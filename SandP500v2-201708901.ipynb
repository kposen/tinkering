{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Modules\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense, Dropout, GaussianNoise, GRU, LSTM, Conv1D, Flatten\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data_file_path = ''\n",
    "data_filename = 'spx_history.csv'\n",
    "processed_data_file = 'processed_data.csv'\n",
    "model_input_file = 'model_input.csv'\n",
    "predictions_file = 'predictions.csv'\n",
    "dt_format = '%Y-%m-%d'\n",
    "\n",
    "# Model\n",
    "model_file = 'model.hdf5'\n",
    "num_epochs = 10000\n",
    "validation_frac = 0.2\n",
    "batch_size = 32\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "dropout = 0.1\n",
    "reg_coeff = 0.0001\n",
    "neurons = 32\n",
    "noise_std = 0.0\n",
    "target_return = 0.20\n",
    "kernel_regularizer = None # l2(reg_coeff)\n",
    "outlier_std = 4\n",
    "span = 2\n",
    "padding = 'valid'\n",
    "# Activation = LeakyReLU(alpha=0.1)\n",
    "leakyalpha = 0.1\n",
    "num_conv_layers = 9\n",
    "\n",
    "# Investment decision\n",
    "window = 2500 # days\n",
    "investment_horizon = 250 # days\n",
    "stride = 1 #days\n",
    "mov_avg_period = 5 # days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_price_data(filename, dt_format):\n",
    "    prices = pd.read_csv(filename,\n",
    "                         delimiter=',',\n",
    "                         header=0,\n",
    "                         names=['date', 'P_close'],\n",
    "                         index_col=0,\n",
    "                         parse_dates=True,\n",
    "                         date_parser=lambda date_str: dt.datetime.strptime(date_str, dt_format))\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_processed_data(prices_df, investment_horizon):\n",
    "    prices_df['log_P'] = prices_df['P_close'].map(np.log)\n",
    "    prices_df['diff'] = prices_df['log_P'].diff(1)\n",
    "    outlier_threshold = outlier_std * prices_df['diff'].std()\n",
    "    prices_df['cleaned'] = prices_df['diff'][prices_df['diff']\n",
    "                                             .subtract(prices_df['diff'].mean())\n",
    "                                             .abs()\n",
    "                                             .lt(outlier_threshold)]\n",
    "    \n",
    "    \n",
    "    pivot = prices_df['cleaned'].median()\n",
    "    scale = prices_df['cleaned'].std()\n",
    "    prices_df['scaled'] = (prices_df['cleaned'] - pivot)/scale\n",
    "\n",
    "    prices_df['smoothed'] = prices_df['log_P'].rolling(mov_avg_period).min()\n",
    "    prices_df['roll_max'] = (prices_df['smoothed']\n",
    "                             .rolling(investment_horizon)\n",
    "                             .max()\n",
    "                             .shift(1-investment_horizon))\n",
    "    prices_df['max_return'] = prices_df['roll_max'].subtract(prices_df['log_P'])\n",
    "\n",
    "    threshold = prices_df['max_return'].mean()\n",
    "#     threshold = np.log(1+target_return)\n",
    "    \n",
    "    print(threshold)\n",
    "\n",
    "    def map_outcome(x):\n",
    "        if x == False:\n",
    "            return 0\n",
    "        elif x == True:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    prices_df['outcome'] = prices_df['max_return'].dropna().gt(threshold).map(map_outcome)\n",
    "    \n",
    "    prices_df.to_csv(processed_data_file)\n",
    "    print(prices_df)\n",
    "    print(prices_df.describe())\n",
    "\n",
    "    return prices_df[['scaled', 'outcome']].dropna(), pivot, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_samples(data_df, window, stride):\n",
    "    time_series = data_df.iloc[:, 0].values\n",
    "    outcomes = data_df.iloc[:, 1].values\n",
    "    x, y = zip(*[(time_series[i-window:i], outcomes[i]) for i in range(window, len(time_series), stride)])\n",
    "    \n",
    "    return np.array(x).reshape(-1, window, 1), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_processed_data(df, filename):\n",
    "    df.to_csv(filename, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(window):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Convolutions\n",
    "    for i in range(num_conv_layers):\n",
    "        if i == 0:\n",
    "            input_shape = (window, 1)\n",
    "        else:\n",
    "            input_shape = (None,)\n",
    "        model.add(Conv1D(neurons, span, kernel_regularizer=kernel_regularizer, padding=padding, input_shape=input_shape))\n",
    "        model.add(LeakyReLU(alpha=leakyalpha))\n",
    "        if i < num_conv_layers-1:\n",
    "            model.add(MaxPooling1D(span))\n",
    "            model.add(Dropout(dropout))\n",
    "#             model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "#     model.add(Flatten())\n",
    "    \n",
    "#     # Recurrents\n",
    "#     model.add(GRU(128, return_sequences=True, go_backwards=True))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(GRU(128, return_sequences=False, go_backwards=False))\n",
    "#     model.add(Dropout(dropout))\n",
    "    \n",
    "    # Dense for final prediction\n",
    "    model.add(Dense(neurons, kernel_regularizer=kernel_regularizer))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=kernel_regularizer))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.138578064736\n",
      "            P_close     log_P      diff   cleaned    scaled  smoothed  \\\n",
      "date                                                                    \n",
      "1927-12-30    17.66  2.871302       NaN       NaN       NaN       NaN   \n",
      "1928-01-03    17.76  2.876949  0.005647  0.005647  0.515028       NaN   \n",
      "1928-01-04    17.72  2.874694 -0.002255 -0.002255 -0.270832       NaN   \n",
      "1928-01-05    17.55  2.865054 -0.009640 -0.009640 -1.005358       NaN   \n",
      "1928-01-06    17.66  2.871302  0.006248  0.006248  0.574873  2.865054   \n",
      "1928-01-09    17.50  2.862201 -0.009101 -0.009101 -0.951781  2.862201   \n",
      "1928-01-10    17.37  2.854745 -0.007456 -0.007456 -0.788169  2.854745   \n",
      "1928-01-11    17.35  2.853593 -0.001152 -0.001152 -0.161157  2.853593   \n",
      "1928-01-12    17.47  2.860485  0.006893  0.006893  0.638961  2.853593   \n",
      "1928-01-13    17.58  2.866762  0.006277  0.006277  0.577709  2.853593   \n",
      "1928-01-16    17.29  2.850128 -0.016634 -0.016634 -1.700935  2.850128   \n",
      "1928-01-17    17.30  2.850707  0.000578  0.000578  0.010935  2.850128   \n",
      "1928-01-18    17.26  2.848392 -0.002315 -0.002315 -0.276802  2.848392   \n",
      "1928-01-19    17.38  2.855320  0.006928  0.006928  0.642524  2.848392   \n",
      "1928-01-20    17.48  2.861057  0.005737  0.005737  0.524049  2.848392   \n",
      "1928-01-23    17.64  2.870169  0.009112  0.009112  0.859667  2.848392   \n",
      "1928-01-24    17.71  2.874129  0.003960  0.003960  0.347326  2.848392   \n",
      "1928-01-25    17.52  2.863343 -0.010786 -0.010786 -1.119375  2.855320   \n",
      "1928-01-26    17.63  2.869602  0.006259  0.006259  0.575933  2.861057   \n",
      "1928-01-27    17.69  2.873000  0.003398  0.003398  0.291341  2.863343   \n",
      "1928-01-30    17.49  2.861629 -0.011370 -0.011370 -1.177444  2.861629   \n",
      "1928-01-31    17.57  2.866193  0.004564  0.004564  0.407321  2.861629   \n",
      "1928-02-01    17.53  2.863914 -0.002279 -0.002279 -0.273260  2.861629   \n",
      "1928-02-02    17.63  2.869602  0.005688  0.005688  0.519181  2.861629   \n",
      "1928-02-03    17.40  2.856470 -0.013132 -0.013132 -1.352649  2.856470   \n",
      "1928-02-06    17.45  2.859340  0.002869  0.002869  0.238820  2.856470   \n",
      "1928-02-07    17.44  2.858766 -0.000573 -0.000573 -0.103585  2.856470   \n",
      "1928-02-08    17.49  2.861629  0.002863  0.002863  0.238166  2.856470   \n",
      "1928-02-09    17.55  2.865054  0.003425  0.003425  0.294041  2.856470   \n",
      "1928-02-10    17.54  2.864484 -0.000570 -0.000570 -0.103260  2.858766   \n",
      "...             ...       ...       ...       ...       ...       ...   \n",
      "2017-06-22  2434.50  7.797497 -0.000456 -0.000456 -0.091910  7.796942   \n",
      "2017-06-23  2438.30  7.799056  0.001560  0.001560  0.108552  7.797497   \n",
      "2017-06-26  2439.07  7.799372  0.000316  0.000316 -0.015169  7.797497   \n",
      "2017-06-27  2419.38  7.791267 -0.008106 -0.008106 -0.852739  7.791267   \n",
      "2017-06-28  2440.69  7.800036  0.008769  0.008769  0.825632  7.791267   \n",
      "2017-06-29  2419.70  7.791399 -0.008637 -0.008637 -0.905623  7.791267   \n",
      "2017-06-30  2423.41  7.792931  0.001532  0.001532  0.105806  7.791267   \n",
      "2017-07-03  2429.01  7.795239  0.002308  0.002308  0.182992  7.791267   \n",
      "2017-07-05  2432.54  7.796691  0.001452  0.001452  0.097863  7.791399   \n",
      "2017-07-06  2409.75  7.787278 -0.009413 -0.009413 -0.982778  7.787278   \n",
      "2017-07-07  2425.18  7.793661  0.006383  0.006383  0.588249  7.787278   \n",
      "2017-07-10  2427.43  7.794588  0.000927  0.000927  0.045660  7.787278   \n",
      "2017-07-11  2425.53  7.793805 -0.000783 -0.000783 -0.124452  7.787278   \n",
      "2017-07-12  2443.25  7.801084  0.007279  0.007279  0.677397  7.787278   \n",
      "2017-07-13  2447.83  7.802957  0.001873  0.001873  0.139694  7.793661   \n",
      "2017-07-14  2459.27  7.807620  0.004663  0.004663  0.417170  7.793805   \n",
      "2017-07-17  2459.14  7.807567 -0.000053 -0.000053 -0.051830  7.793805   \n",
      "2017-07-18  2460.61  7.808165  0.000598  0.000598  0.012864  7.801084   \n",
      "2017-07-19  2473.83  7.813523  0.005358  0.005358  0.486356  7.802957   \n",
      "2017-07-20  2473.45  7.813369 -0.000154 -0.000154 -0.061851  7.807567   \n",
      "2017-07-21  2472.54  7.813001 -0.000368 -0.000368 -0.083171  7.807567   \n",
      "2017-07-24  2469.91  7.811937 -0.001064 -0.001064 -0.152422  7.808165   \n",
      "2017-07-25  2477.13  7.814856  0.002919  0.002919  0.243741  7.811937   \n",
      "2017-07-26  2477.83  7.815138  0.000283  0.000283 -0.018471  7.811937   \n",
      "2017-07-27  2475.42  7.814165 -0.000973 -0.000973 -0.143356  7.811937   \n",
      "2017-07-28  2472.10  7.812823 -0.001342 -0.001342 -0.180055  7.811937   \n",
      "2017-07-31  2470.30  7.812095 -0.000728 -0.000728 -0.119017  7.812095   \n",
      "2017-08-01  2476.35  7.814541  0.002446  0.002446  0.196715  7.812095   \n",
      "2017-08-02  2477.57  7.815034  0.000493  0.000493  0.002415  7.812095   \n",
      "2017-08-03  2472.16  7.812848 -0.002186 -0.002186 -0.263988  7.812095   \n",
      "\n",
      "            roll_max  max_return  outcome  \n",
      "date                                       \n",
      "1927-12-30       NaN         NaN      NaN  \n",
      "1928-01-03       NaN         NaN      NaN  \n",
      "1928-01-04       NaN         NaN      NaN  \n",
      "1928-01-05       NaN         NaN      NaN  \n",
      "1928-01-06  3.177220    0.305918      1.0  \n",
      "1928-01-09  3.188417    0.326216      1.0  \n",
      "1928-01-10  3.188417    0.333672      1.0  \n",
      "1928-01-11  3.188417    0.334824      1.0  \n",
      "1928-01-12  3.188417    0.327931      1.0  \n",
      "1928-01-13  3.188417    0.321655      1.0  \n",
      "1928-01-16  3.188417    0.338288      1.0  \n",
      "1928-01-17  3.188417    0.337710      1.0  \n",
      "1928-01-18  3.188417    0.340025      1.0  \n",
      "1928-01-19  3.188417    0.333096      1.0  \n",
      "1928-01-20  3.188417    0.327359      1.0  \n",
      "1928-01-23  3.188417    0.318248      1.0  \n",
      "1928-01-24  3.201119    0.326990      1.0  \n",
      "1928-01-25  3.206398    0.343055      1.0  \n",
      "1928-01-26  3.209633    0.340031      1.0  \n",
      "1928-01-27  3.213260    0.340261      1.0  \n",
      "1928-01-30  3.221273    0.359644      1.0  \n",
      "1928-01-31  3.221273    0.355080      1.0  \n",
      "1928-02-01  3.221273    0.357359      1.0  \n",
      "1928-02-02  3.232779    0.363177      1.0  \n",
      "1928-02-03  3.232779    0.376309      1.0  \n",
      "1928-02-06  3.233173    0.373833      1.0  \n",
      "1928-02-07  3.236716    0.377949      1.0  \n",
      "1928-02-08  3.238286    0.376657      1.0  \n",
      "1928-02-09  3.238286    0.373232      1.0  \n",
      "1928-02-10  3.238286    0.373802      1.0  \n",
      "...              ...         ...      ...  \n",
      "2017-06-22       NaN         NaN      NaN  \n",
      "2017-06-23       NaN         NaN      NaN  \n",
      "2017-06-26       NaN         NaN      NaN  \n",
      "2017-06-27       NaN         NaN      NaN  \n",
      "2017-06-28       NaN         NaN      NaN  \n",
      "2017-06-29       NaN         NaN      NaN  \n",
      "2017-06-30       NaN         NaN      NaN  \n",
      "2017-07-03       NaN         NaN      NaN  \n",
      "2017-07-05       NaN         NaN      NaN  \n",
      "2017-07-06       NaN         NaN      NaN  \n",
      "2017-07-07       NaN         NaN      NaN  \n",
      "2017-07-10       NaN         NaN      NaN  \n",
      "2017-07-11       NaN         NaN      NaN  \n",
      "2017-07-12       NaN         NaN      NaN  \n",
      "2017-07-13       NaN         NaN      NaN  \n",
      "2017-07-14       NaN         NaN      NaN  \n",
      "2017-07-17       NaN         NaN      NaN  \n",
      "2017-07-18       NaN         NaN      NaN  \n",
      "2017-07-19       NaN         NaN      NaN  \n",
      "2017-07-20       NaN         NaN      NaN  \n",
      "2017-07-21       NaN         NaN      NaN  \n",
      "2017-07-24       NaN         NaN      NaN  \n",
      "2017-07-25       NaN         NaN      NaN  \n",
      "2017-07-26       NaN         NaN      NaN  \n",
      "2017-07-27       NaN         NaN      NaN  \n",
      "2017-07-28       NaN         NaN      NaN  \n",
      "2017-07-31       NaN         NaN      NaN  \n",
      "2017-08-01       NaN         NaN      NaN  \n",
      "2017-08-02       NaN         NaN      NaN  \n",
      "2017-08-03       NaN         NaN      NaN  \n",
      "\n",
      "[22504 rows x 9 columns]\n",
      "            P_close         log_P          diff       cleaned        scaled  \\\n",
      "count  22504.000000  22504.000000  22503.000000  22328.000000  22328.000000   \n",
      "mean     399.850251      4.734416      0.000220      0.000300     -0.016703   \n",
      "std      573.245719      1.723863      0.011763      0.010054      1.000000   \n",
      "min        4.400000      1.481605     -0.228997     -0.046789     -4.700143   \n",
      "25%       22.987500      3.134951     -0.004572     -0.004484     -0.492543   \n",
      "50%       96.700000      4.571613      0.000460      0.000468      0.000000   \n",
      "75%      503.140000      6.220868      0.005369      0.005334      0.483910   \n",
      "max     2477.830000      7.815138      0.153661      0.047108      4.638755   \n",
      "\n",
      "           smoothed      roll_max    max_return       outcome  \n",
      "count  22500.000000  22251.000000  22251.000000  22251.000000  \n",
      "mean       4.723103      4.839718      0.138578      0.427531  \n",
      "std        1.725022      1.689655      0.111641      0.494732  \n",
      "min        1.481605      2.169054     -0.056723      0.000000  \n",
      "25%        3.124125      3.247658      0.055959      0.000000  \n",
      "50%        4.563671      4.653770      0.116761      0.000000  \n",
      "75%        6.216027      6.298912      0.202656      1.000000  \n",
      "max        7.812095      7.812095      0.988453      1.000000  \n"
     ]
    }
   ],
   "source": [
    "full_filename = os.path.join(data_file_path, data_filename)\n",
    "data = get_price_data(full_filename, dt_format)\n",
    "processed_data, midpoint, scale = get_processed_data(data, investment_horizon)\n",
    "X, y = get_samples(processed_data, window, stride)\n",
    "save_processed_data(processed_data, model_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (19576, 2500, 1)\n",
      "y.shape:  (19576, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X.shape: ', X.shape)\n",
    "print('y.shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_data = pd.DataFrame(X.reshape(-1, window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6     \\\n",
      "0      0.574873 -0.951781 -0.788169 -0.161157  0.638961  0.577709 -1.700935   \n",
      "1     -0.951781 -0.788169 -0.161157  0.638961  0.577709 -1.700935  0.010935   \n",
      "2     -0.788169 -0.161157  0.638961  0.577709 -1.700935  0.010935 -0.276802   \n",
      "3     -0.161157  0.638961  0.577709 -1.700935  0.010935 -0.276802  0.642524   \n",
      "4      0.638961  0.577709 -1.700935  0.010935 -0.276802  0.642524  0.524049   \n",
      "5      0.577709 -1.700935  0.010935 -0.276802  0.642524  0.524049  0.859667   \n",
      "6     -1.700935  0.010935 -0.276802  0.642524  0.524049  0.859667  0.347326   \n",
      "7      0.010935 -0.276802  0.642524  0.524049  0.859667  0.347326 -1.119375   \n",
      "8     -0.276802  0.642524  0.524049  0.859667  0.347326 -1.119375  0.575933   \n",
      "9      0.642524  0.524049  0.859667  0.347326 -1.119375  0.575933  0.291341   \n",
      "10     0.524049  0.859667  0.347326 -1.119375  0.575933  0.291341 -1.177444   \n",
      "11     0.859667  0.347326 -1.119375  0.575933  0.291341 -1.177444  0.407321   \n",
      "12     0.347326 -1.119375  0.575933  0.291341 -1.177444  0.407321 -0.273260   \n",
      "13    -1.119375  0.575933  0.291341 -1.177444  0.407321 -0.273260  0.519181   \n",
      "14     0.575933  0.291341 -1.177444  0.407321 -0.273260  0.519181 -1.352649   \n",
      "15     0.291341 -1.177444  0.407321 -0.273260  0.519181 -1.352649  0.238820   \n",
      "16    -1.177444  0.407321 -0.273260  0.519181 -1.352649  0.238820 -0.103585   \n",
      "17     0.407321 -0.273260  0.519181 -1.352649  0.238820 -0.103585  0.238166   \n",
      "18    -0.273260  0.519181 -1.352649  0.238820 -0.103585  0.238166  0.294041   \n",
      "19     0.519181 -1.352649  0.238820 -0.103585  0.238166  0.294041 -0.103260   \n",
      "20    -1.352649  0.238820 -0.103585  0.238166  0.294041 -0.103260 -0.615237   \n",
      "21     0.238820 -0.103585  0.238166  0.294041 -0.103260 -0.615237 -0.274952   \n",
      "22    -0.103585  0.238166  0.294041 -0.103260 -0.615237 -0.274952 -0.332786   \n",
      "23     0.238166  0.294041 -0.103260 -0.615237 -0.274952 -0.332786 -1.839719   \n",
      "24     0.294041 -0.103260 -0.615237 -0.274952 -0.332786 -1.839719 -0.573277   \n",
      "25    -0.103260 -0.615237 -0.274952 -0.332786 -1.839719 -0.573277  0.887871   \n",
      "26    -0.615237 -0.274952 -0.332786 -1.839719 -0.573277  0.887871  0.069618   \n",
      "27    -0.274952 -0.332786 -1.839719 -0.573277  0.887871  0.069618  0.243312   \n",
      "28    -0.332786 -1.839719 -0.573277  0.887871  0.069618  0.243312 -0.452647   \n",
      "29    -1.839719 -0.573277  0.887871  0.069618  0.243312 -0.452647  0.243650   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19546 -1.074288  0.468209  2.043338 -0.413045 -0.956675 -0.048176  0.917570   \n",
      "19547  0.468209  2.043338 -0.413045 -0.956675 -0.048176  0.917570 -0.572180   \n",
      "19548  2.043338 -0.413045 -0.956675 -0.048176  0.917570 -0.572180 -0.134444   \n",
      "19549 -0.413045 -0.956675 -0.048176  0.917570 -0.572180 -0.134444  0.436561   \n",
      "19550 -0.956675 -0.048176  0.917570 -0.572180 -0.134444  0.436561 -0.954181   \n",
      "19551 -0.048176  0.917570 -0.572180 -0.134444  0.436561 -0.954181  0.497709   \n",
      "19552  0.917570 -0.572180 -0.134444  0.436561 -0.954181  0.497709  2.075465   \n",
      "19553 -0.572180 -0.134444  0.436561 -0.954181  0.497709  2.075465 -0.255419   \n",
      "19554 -0.134444  0.436561 -0.954181  0.497709  2.075465 -0.255419  0.732604   \n",
      "19555  0.436561 -0.954181  0.497709  2.075465 -0.255419  0.732604 -0.770170   \n",
      "19556 -0.954181  0.497709  2.075465 -0.255419  0.732604 -0.770170  0.201197   \n",
      "19557  0.497709  2.075465 -0.255419  0.732604 -0.770170  0.201197 -0.720194   \n",
      "19558  2.075465 -0.255419  0.732604 -0.770170  0.201197 -0.720194  0.099505   \n",
      "19559 -0.255419  0.732604 -0.770170  0.201197 -0.720194  0.099505  0.359118   \n",
      "19560  0.732604 -0.770170  0.201197 -0.720194  0.099505  0.359118 -1.140542   \n",
      "19561 -0.770170  0.201197 -0.720194  0.099505  0.359118 -1.140542 -1.343871   \n",
      "19562  0.201197 -0.720194  0.099505  0.359118 -1.140542 -1.343871 -0.535343   \n",
      "19563 -0.720194  0.099505  0.359118 -1.140542 -1.343871 -0.535343 -0.184247   \n",
      "19564  0.099505  0.359118 -1.140542 -1.343871 -0.535343 -0.184247  0.144188   \n",
      "19565  0.359118 -1.140542 -1.343871 -0.535343 -0.184247  0.144188  1.781984   \n",
      "19566 -1.140542 -1.343871 -0.535343 -0.184247  0.144188  1.781984 -0.893328   \n",
      "19567 -1.343871 -0.535343 -0.184247  0.144188  1.781984 -0.893328 -0.752939   \n",
      "19568 -0.535343 -0.184247  0.144188  1.781984 -0.893328 -0.752939  1.593355   \n",
      "19569 -0.184247  0.144188  1.781984 -0.893328 -0.752939  1.593355  0.580113   \n",
      "19570  0.144188  1.781984 -0.893328 -0.752939  1.593355  0.580113 -0.084203   \n",
      "19571  1.781984 -0.893328 -0.752939  1.593355  0.580113 -0.084203 -0.455158   \n",
      "19572 -0.893328 -0.752939  1.593355  0.580113 -0.084203 -0.455158  1.154738   \n",
      "19573 -0.752939  1.593355  0.580113 -0.084203 -0.455158  1.154738 -0.193705   \n",
      "19574  1.593355  0.580113 -0.084203 -0.455158  1.154738 -0.193705 -0.494759   \n",
      "19575  0.580113 -0.084203 -0.455158  1.154738 -0.193705 -0.494759  0.548748   \n",
      "\n",
      "           7         8         9       ...         2490      2491      2492  \\\n",
      "0      0.010935 -0.276802  0.642524    ...    -1.305565  3.275962 -0.455870   \n",
      "1     -0.276802  0.642524  0.524049    ...     3.275962 -0.455870  0.260558   \n",
      "2      0.642524  0.524049  0.859667    ...    -0.455870  0.260558  2.077190   \n",
      "3      0.524049  0.859667  0.347326    ...     0.260558  2.077190 -1.457364   \n",
      "4      0.859667  0.347326 -1.119375    ...     2.077190 -1.457364 -2.929647   \n",
      "5      0.347326 -1.119375  0.575933    ...    -1.457364 -2.929647  1.096051   \n",
      "6     -1.119375  0.575933  0.291341    ...    -2.929647  1.096051  0.365694   \n",
      "7      0.575933  0.291341 -1.177444    ...     1.096051  0.365694  0.874826   \n",
      "8      0.291341 -1.177444  0.407321    ...     0.365694  0.874826 -0.967971   \n",
      "9     -1.177444  0.407321 -0.273260    ...     0.874826 -0.967971  2.554444   \n",
      "10     0.407321 -0.273260  0.519181    ...    -0.967971  2.554444  2.488153   \n",
      "11    -0.273260  0.519181 -1.352649    ...     2.554444  2.488153  2.696040   \n",
      "12     0.519181 -1.352649  0.238820    ...     2.488153  2.696040  1.029641   \n",
      "13    -1.352649  0.238820 -0.103585    ...     2.696040  1.029641  0.929830   \n",
      "14     0.238820 -0.103585  0.238166    ...     1.029641  0.929830 -0.134941   \n",
      "15    -0.103585  0.238166  0.294041    ...     0.929830 -0.134941  4.024155   \n",
      "16     0.238166  0.294041 -0.103260    ...    -0.134941  4.024155 -1.413729   \n",
      "17     0.294041 -0.103260 -0.615237    ...     4.024155 -1.413729  2.920046   \n",
      "18    -0.103260 -0.615237 -0.274952    ...    -1.413729  2.920046  0.536279   \n",
      "19    -0.615237 -0.274952 -0.332786    ...     2.920046  0.536279  1.272971   \n",
      "20    -0.274952 -0.332786 -1.839719    ...     0.536279  1.272971 -0.457052   \n",
      "21    -0.332786 -1.839719 -0.573277    ...     1.272971 -0.457052 -1.705645   \n",
      "22    -1.839719 -0.573277  0.887871    ...    -0.457052 -1.705645 -1.394049   \n",
      "23    -0.573277  0.887871  0.069618    ...    -1.705645 -1.394049  2.547796   \n",
      "24     0.887871  0.069618  0.243312    ...    -1.394049  2.547796 -0.792830   \n",
      "25     0.069618  0.243312 -0.452647    ...     2.547796 -0.792830 -0.966335   \n",
      "26     0.243312 -0.452647  0.243650    ...    -0.792830 -0.966335  1.288469   \n",
      "27    -0.452647  0.243650  0.531344    ...    -0.966335  1.288469  2.812819   \n",
      "28     0.243650  0.531344  0.183657    ...     1.288469  2.812819  2.497684   \n",
      "29     0.531344  0.183657 -0.046572    ...     2.812819  2.497684 -1.151685   \n",
      "...         ...       ...       ...    ...          ...       ...       ...   \n",
      "19546 -0.572180 -0.134444  0.436561    ...    -0.856994 -0.225649 -0.229813   \n",
      "19547 -0.134444  0.436561 -0.954181    ...    -0.225649 -0.229813  0.264545   \n",
      "19548  0.436561 -0.954181  0.497709    ...    -0.229813  0.264545 -0.371135   \n",
      "19549 -0.954181  0.497709  2.075465    ...     0.264545 -0.371135  0.529432   \n",
      "19550  0.497709  2.075465 -0.255419    ...    -0.371135  0.529432  0.222806   \n",
      "19551  2.075465 -0.255419  0.732604    ...     0.529432  0.222806 -0.210974   \n",
      "19552 -0.255419  0.732604 -0.770170    ...     0.222806 -0.210974  1.273798   \n",
      "19553  0.732604 -0.770170  0.201197    ...    -0.210974  1.273798 -3.684864   \n",
      "19554 -0.770170  0.201197 -0.720194    ...     1.273798 -3.684864 -1.862919   \n",
      "19555  0.201197 -0.720194  0.099505    ...    -3.684864 -1.862919  1.705316   \n",
      "19556 -0.720194  0.099505  0.359118    ...    -1.862919  1.705316  1.633214   \n",
      "19557  0.099505  0.359118 -1.140542    ...     1.705316  1.633214  1.293522   \n",
      "19558  0.359118 -1.140542 -1.343871    ...     1.633214  1.293522  0.147053   \n",
      "19559 -1.140542 -1.343871 -0.535343    ...     1.293522  0.147053 -0.729963   \n",
      "19560 -1.343871 -0.535343 -0.184247    ...     0.147053 -0.729963  0.484412   \n",
      "19561 -0.535343 -0.184247  0.144188    ...    -0.729963  0.484412 -0.133293   \n",
      "19562 -0.184247  0.144188  1.781984    ...     0.484412 -0.133293  1.459058   \n",
      "19563  0.144188  1.781984 -0.893328    ...    -0.133293  1.459058  0.291869   \n",
      "19564  1.781984 -0.893328 -0.752939    ...     1.459058  0.291869  0.648135   \n",
      "19565 -0.893328 -0.752939  1.593355    ...     0.291869  0.648135 -0.033171   \n",
      "19566 -0.752939  1.593355  0.580113    ...     0.648135 -0.033171  0.475130   \n",
      "19567  1.593355  0.580113 -0.084203    ...    -0.033171  0.475130 -0.139007   \n",
      "19568  0.580113 -0.084203 -0.455158    ...     0.475130 -0.139007  0.190091   \n",
      "19569 -0.084203 -0.455158  1.154738    ...    -0.139007  0.190091 -0.189422   \n",
      "19570 -0.455158  1.154738 -0.193705    ...     0.190091 -0.189422  0.377244   \n",
      "19571  1.154738 -0.193705 -0.494759    ...    -0.189422  0.377244 -0.406517   \n",
      "19572 -0.193705 -0.494759  0.548748    ...     0.377244 -0.406517  0.405328   \n",
      "19573 -0.494759  0.548748  0.087137    ...    -0.406517  0.405328 -0.346541   \n",
      "19574  0.548748  0.087137 -0.117292    ...     0.405328 -0.346541 -0.014471   \n",
      "19575  0.087137 -0.117292 -0.326056    ...    -0.346541 -0.014471 -0.165856   \n",
      "\n",
      "           2493      2494      2495      2496      2497      2498      2499  \n",
      "0      0.260558  2.077190 -1.457364 -2.929647  1.096051  0.365694  0.874826  \n",
      "1      2.077190 -1.457364 -2.929647  1.096051  0.365694  0.874826 -0.967971  \n",
      "2     -1.457364 -2.929647  1.096051  0.365694  0.874826 -0.967971  2.554444  \n",
      "3     -2.929647  1.096051  0.365694  0.874826 -0.967971  2.554444  2.488153  \n",
      "4      1.096051  0.365694  0.874826 -0.967971  2.554444  2.488153  2.696040  \n",
      "5      0.365694  0.874826 -0.967971  2.554444  2.488153  2.696040  1.029641  \n",
      "6      0.874826 -0.967971  2.554444  2.488153  2.696040  1.029641  0.929830  \n",
      "7     -0.967971  2.554444  2.488153  2.696040  1.029641  0.929830 -0.134941  \n",
      "8      2.554444  2.488153  2.696040  1.029641  0.929830 -0.134941  4.024155  \n",
      "9      2.488153  2.696040  1.029641  0.929830 -0.134941  4.024155 -1.413729  \n",
      "10     2.696040  1.029641  0.929830 -0.134941  4.024155 -1.413729  2.920046  \n",
      "11     1.029641  0.929830 -0.134941  4.024155 -1.413729  2.920046  0.536279  \n",
      "12     0.929830 -0.134941  4.024155 -1.413729  2.920046  0.536279  1.272971  \n",
      "13    -0.134941  4.024155 -1.413729  2.920046  0.536279  1.272971 -0.457052  \n",
      "14     4.024155 -1.413729  2.920046  0.536279  1.272971 -0.457052 -1.705645  \n",
      "15    -1.413729  2.920046  0.536279  1.272971 -0.457052 -1.705645 -1.394049  \n",
      "16     2.920046  0.536279  1.272971 -0.457052 -1.705645 -1.394049  2.547796  \n",
      "17     0.536279  1.272971 -0.457052 -1.705645 -1.394049  2.547796 -0.792830  \n",
      "18     1.272971 -0.457052 -1.705645 -1.394049  2.547796 -0.792830 -0.966335  \n",
      "19    -0.457052 -1.705645 -1.394049  2.547796 -0.792830 -0.966335  1.288469  \n",
      "20    -1.705645 -1.394049  2.547796 -0.792830 -0.966335  1.288469  2.812819  \n",
      "21    -1.394049  2.547796 -0.792830 -0.966335  1.288469  2.812819  2.497684  \n",
      "22     2.547796 -0.792830 -0.966335  1.288469  2.812819  2.497684 -1.151685  \n",
      "23    -0.792830 -0.966335  1.288469  2.812819  2.497684 -1.151685  0.032773  \n",
      "24    -0.966335  1.288469  2.812819  2.497684 -1.151685  0.032773  0.032710  \n",
      "25     1.288469  2.812819  2.497684 -1.151685  0.032773  0.032710  1.915209  \n",
      "26     2.812819  2.497684 -1.151685  0.032773  0.032710  1.915209 -1.455143  \n",
      "27     2.497684 -1.151685  0.032773  0.032710  1.915209 -1.455143 -3.006169  \n",
      "28    -1.151685  0.032773  0.032710  1.915209 -1.455143 -3.006169  1.803509  \n",
      "29     0.032773  0.032710  1.915209 -1.455143 -3.006169  1.803509 -1.249236  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "19546  0.264545 -0.371135  0.529432  0.222806 -0.210974  1.273798 -3.684864  \n",
      "19547 -0.371135  0.529432  0.222806 -0.210974  1.273798 -3.684864 -1.862919  \n",
      "19548  0.529432  0.222806 -0.210974  1.273798 -3.684864 -1.862919  1.705316  \n",
      "19549  0.222806 -0.210974  1.273798 -3.684864 -1.862919  1.705316  1.633214  \n",
      "19550 -0.210974  1.273798 -3.684864 -1.862919  1.705316  1.633214  1.293522  \n",
      "19551  1.273798 -3.684864 -1.862919  1.705316  1.633214  1.293522  0.147053  \n",
      "19552 -3.684864 -1.862919  1.705316  1.633214  1.293522  0.147053 -0.729963  \n",
      "19553 -1.862919  1.705316  1.633214  1.293522  0.147053 -0.729963  0.484412  \n",
      "19554  1.705316  1.633214  1.293522  0.147053 -0.729963  0.484412 -0.133293  \n",
      "19555  1.633214  1.293522  0.147053 -0.729963  0.484412 -0.133293  1.459058  \n",
      "19556  1.293522  0.147053 -0.729963  0.484412 -0.133293  1.459058  0.291869  \n",
      "19557  0.147053 -0.729963  0.484412 -0.133293  1.459058  0.291869  0.648135  \n",
      "19558 -0.729963  0.484412 -0.133293  1.459058  0.291869  0.648135 -0.033171  \n",
      "19559  0.484412 -0.133293  1.459058  0.291869  0.648135 -0.033171  0.475130  \n",
      "19560 -0.133293  1.459058  0.291869  0.648135 -0.033171  0.475130 -0.139007  \n",
      "19561  1.459058  0.291869  0.648135 -0.033171  0.475130 -0.139007  0.190091  \n",
      "19562  0.291869  0.648135 -0.033171  0.475130 -0.139007  0.190091 -0.189422  \n",
      "19563  0.648135 -0.033171  0.475130 -0.139007  0.190091 -0.189422  0.377244  \n",
      "19564 -0.033171  0.475130 -0.139007  0.190091 -0.189422  0.377244 -0.406517  \n",
      "19565  0.475130 -0.139007  0.190091 -0.189422  0.377244 -0.406517  0.405328  \n",
      "19566 -0.139007  0.190091 -0.189422  0.377244 -0.406517  0.405328 -0.346541  \n",
      "19567  0.190091 -0.189422  0.377244 -0.406517  0.405328 -0.346541 -0.014471  \n",
      "19568 -0.189422  0.377244 -0.406517  0.405328 -0.346541 -0.014471 -0.165856  \n",
      "19569  0.377244 -0.406517  0.405328 -0.346541 -0.014471 -0.165856  0.113053  \n",
      "19570 -0.406517  0.405328 -0.346541 -0.014471 -0.165856  0.113053  0.115542  \n",
      "19571  0.405328 -0.346541 -0.014471 -0.165856  0.113053  0.115542 -0.172944  \n",
      "19572 -0.346541 -0.014471 -0.165856  0.113053  0.115542 -0.172944 -0.681312  \n",
      "19573 -0.014471 -0.165856  0.113053  0.115542 -0.172944 -0.681312  0.264639  \n",
      "19574 -0.165856  0.113053  0.115542 -0.172944 -0.681312  0.264639 -0.025431  \n",
      "19575  0.113053  0.115542 -0.172944 -0.681312  0.264639 -0.025431  0.805458  \n",
      "\n",
      "[19576 rows x 2500 columns]\n"
     ]
    }
   ],
   "source": [
    "print(check_data)\n",
    "(check_data[-500:] * scale + midpoint).to_csv('samples.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_close</th>\n",
       "      <th>log_P</th>\n",
       "      <th>diff</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>scaled</th>\n",
       "      <th>smoothed</th>\n",
       "      <th>roll_max</th>\n",
       "      <th>max_return</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22504.000000</td>\n",
       "      <td>22504.000000</td>\n",
       "      <td>22503.000000</td>\n",
       "      <td>22328.000000</td>\n",
       "      <td>22328.000000</td>\n",
       "      <td>22500.000000</td>\n",
       "      <td>22251.000000</td>\n",
       "      <td>22251.000000</td>\n",
       "      <td>22251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>399.850251</td>\n",
       "      <td>4.734416</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-0.016703</td>\n",
       "      <td>4.723103</td>\n",
       "      <td>4.839718</td>\n",
       "      <td>0.138578</td>\n",
       "      <td>0.427531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>573.245719</td>\n",
       "      <td>1.723863</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.725022</td>\n",
       "      <td>1.689655</td>\n",
       "      <td>0.111641</td>\n",
       "      <td>0.494732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.481605</td>\n",
       "      <td>-0.228997</td>\n",
       "      <td>-0.046789</td>\n",
       "      <td>-4.700143</td>\n",
       "      <td>1.481605</td>\n",
       "      <td>2.169054</td>\n",
       "      <td>-0.056723</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.987500</td>\n",
       "      <td>3.134951</td>\n",
       "      <td>-0.004572</td>\n",
       "      <td>-0.004484</td>\n",
       "      <td>-0.492543</td>\n",
       "      <td>3.124125</td>\n",
       "      <td>3.247658</td>\n",
       "      <td>0.055959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.700000</td>\n",
       "      <td>4.571613</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.563671</td>\n",
       "      <td>4.653770</td>\n",
       "      <td>0.116761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>503.140000</td>\n",
       "      <td>6.220868</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>0.483910</td>\n",
       "      <td>6.216027</td>\n",
       "      <td>6.298912</td>\n",
       "      <td>0.202656</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2477.830000</td>\n",
       "      <td>7.815138</td>\n",
       "      <td>0.153661</td>\n",
       "      <td>0.047108</td>\n",
       "      <td>4.638755</td>\n",
       "      <td>7.812095</td>\n",
       "      <td>7.812095</td>\n",
       "      <td>0.988453</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            P_close         log_P          diff       cleaned        scaled  \\\n",
       "count  22504.000000  22504.000000  22503.000000  22328.000000  22328.000000   \n",
       "mean     399.850251      4.734416      0.000220      0.000300     -0.016703   \n",
       "std      573.245719      1.723863      0.011763      0.010054      1.000000   \n",
       "min        4.400000      1.481605     -0.228997     -0.046789     -4.700143   \n",
       "25%       22.987500      3.134951     -0.004572     -0.004484     -0.492543   \n",
       "50%       96.700000      4.571613      0.000460      0.000468      0.000000   \n",
       "75%      503.140000      6.220868      0.005369      0.005334      0.483910   \n",
       "max     2477.830000      7.815138      0.153661      0.047108      4.638755   \n",
       "\n",
       "           smoothed      roll_max    max_return       outcome  \n",
       "count  22500.000000  22251.000000  22251.000000  22251.000000  \n",
       "mean       4.723103      4.839718      0.138578      0.427531  \n",
       "std        1.725022      1.689655      0.111641      0.494732  \n",
       "min        1.481605      2.169054     -0.056723      0.000000  \n",
       "25%        3.124125      3.247658      0.055959      0.000000  \n",
       "50%        4.563671      4.653770      0.116761      0.000000  \n",
       "75%        6.216027      6.298912      0.202656      1.000000  \n",
       "max        7.812095      7.812095      0.988453      1.000000  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSABaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2858d2e15c0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSABaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2858d2e15c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acf(processed_data['scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_52 (Conv1D)           (None, 2499, 32)          96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)   (None, 2499, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1249, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1248, 32)          2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)   (None, 1248, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 624, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 623, 32)           2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 623, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 311, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 310, 32)           2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 310, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 155, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 154, 32)           2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)   (None, 154, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 77, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 76, 32)            2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)   (None, 76, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 38, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 37, 32)            2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)   (None, 37, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 17, 32)            2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)   (None, 17, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 7, 32)             2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_43 (GaussianN (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 17,825\n",
      "Trainable params: 17,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(window)\n",
    "model.summary()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15660 samples, validate on 3916 samples\n",
      "Epoch 1/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.6672 - acc: 0.5787Epoch 00000: val_loss improved from inf to 0.68343, saving model to model.hdf5\n",
      "15660/15660 [==============================] - 98s - loss: 0.6672 - acc: 0.5787 - val_loss: 0.6834 - val_acc: 0.5666\n",
      "Epoch 2/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.5603 - acc: 0.7088Epoch 00001: val_loss did not improve\n",
      "15660/15660 [==============================] - 101s - loss: 0.5602 - acc: 0.7089 - val_loss: 0.7968 - val_acc: 0.5301\n",
      "Epoch 3/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8158Epoch 00002: val_loss did not improve\n",
      "15660/15660 [==============================] - 95s - loss: 0.4189 - acc: 0.8158 - val_loss: 1.0800 - val_acc: 0.4890\n",
      "Epoch 4/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.3123 - acc: 0.8691Epoch 00003: val_loss did not improve\n",
      "15660/15660 [==============================] - 96s - loss: 0.3122 - acc: 0.8691 - val_loss: 1.3046 - val_acc: 0.5005\n",
      "Epoch 5/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9025Epoch 00004: val_loss did not improve\n",
      "15660/15660 [==============================] - 96s - loss: 0.2412 - acc: 0.9026 - val_loss: 2.0223 - val_acc: 0.4768\n",
      "Epoch 6/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9236Epoch 00005: val_loss did not improve\n",
      "15660/15660 [==============================] - 97s - loss: 0.1959 - acc: 0.9236 - val_loss: 2.6479 - val_acc: 0.4773\n",
      "Epoch 7/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9384Epoch 00006: val_loss did not improve\n",
      "15660/15660 [==============================] - 96s - loss: 0.1628 - acc: 0.9384 - val_loss: 2.2975 - val_acc: 0.4987\n",
      "Epoch 8/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9466Epoch 00007: val_loss did not improve\n",
      "15660/15660 [==============================] - 99s - loss: 0.1469 - acc: 0.9466 - val_loss: 2.6396 - val_acc: 0.4737\n",
      "Epoch 9/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9518Epoch 00008: val_loss did not improve\n",
      "15660/15660 [==============================] - 97s - loss: 0.1270 - acc: 0.9518 - val_loss: 2.1781 - val_acc: 0.5258\n",
      "Epoch 10/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9611Epoch 00009: val_loss did not improve\n",
      "15660/15660 [==============================] - 95s - loss: 0.1103 - acc: 0.9611 - val_loss: 3.1273 - val_acc: 0.5184\n",
      "Epoch 11/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9613Epoch 00010: val_loss did not improve\n",
      "15660/15660 [==============================] - 98s - loss: 0.1093 - acc: 0.9613 - val_loss: 3.0271 - val_acc: 0.5306\n",
      "Epoch 12/10000\n",
      "15648/15660 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9678Epoch 00011: val_loss did not improve\n",
      "15660/15660 [==============================] - 106s - loss: 0.0912 - acc: 0.9678 - val_loss: 2.7958 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_file, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "training_history = model.fit(X, y, batch_size=batch_size, epochs=num_epochs, verbose=1,\n",
    "          callbacks=[checkpointer, earlystopper], validation_split=validation_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19576/19576 [==============================] - 51s    \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19576, 1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.957600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.482507e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.613859e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.305580e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.900848e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.546875e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.983435e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  1.957600e+04\n",
       "mean   4.482507e-01\n",
       "std    4.613859e-01\n",
       "min    1.305580e-08\n",
       "25%    3.900848e-03\n",
       "50%    1.546875e-01\n",
       "75%    9.983435e-01\n",
       "max    1.000000e+00"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(predictions).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(predictions.squeeze()).to_csv(predictions_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
