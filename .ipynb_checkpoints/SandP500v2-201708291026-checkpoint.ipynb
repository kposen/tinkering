{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Modules\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense, Dropout, GaussianNoise, GRU, LSTM, Conv1D, Flatten\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data_file_path = ''\n",
    "data_filename = 'spx_history.csv'\n",
    "processed_data_file = 'processed_data.csv'\n",
    "model_input_file = 'model_input.csv'\n",
    "predictions_file = 'predictions.csv'\n",
    "dt_format = '%Y-%m-%d'\n",
    "\n",
    "# Model\n",
    "model_file = 'model.hdf5'\n",
    "num_epochs = 10000\n",
    "validation_frac = 0.2\n",
    "batch_size = 256\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "dropout = 0.5\n",
    "noise_std = 0.25\n",
    "reg_coeff = 0.0001\n",
    "init_neurons = 8\n",
    "neuron_multiplier = 2\n",
    "target_return = 0.20\n",
    "kernel_regularizer = None # l2(reg_coeff)\n",
    "outlier_std = 4\n",
    "conv_span = 2\n",
    "pooling_span = 2\n",
    "padding = 'valid'\n",
    "# Activation = LeakyReLU(alpha=0.1)\n",
    "leakyalpha = 0.1\n",
    "\n",
    "# Investment decision\n",
    "window = 2500 # days\n",
    "investment_horizon = 250 # days\n",
    "stride = 1 #days\n",
    "mov_avg_period = 5 # days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_price_data(filename, dt_format):\n",
    "    prices = pd.read_csv(filename,\n",
    "                         delimiter=',',\n",
    "                         header=0,\n",
    "                         names=['date', 'P_close'],\n",
    "                         index_col=0,\n",
    "                         parse_dates=True,\n",
    "                         date_parser=lambda date_str: dt.datetime.strptime(date_str, dt_format))\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_processed_data(prices_df, investment_horizon):\n",
    "    prices_df['log_P'] = prices_df['P_close'].map(np.log)\n",
    "    prices_df['diff'] = prices_df['log_P'].diff(1)\n",
    "    outlier_threshold = outlier_std * prices_df['diff'].std()\n",
    "    prices_df['cleaned'] = prices_df['diff'][prices_df['diff']\n",
    "                                             .subtract(prices_df['diff'].mean())\n",
    "                                             .abs()\n",
    "                                             .lt(outlier_threshold)]\n",
    "    \n",
    "    \n",
    "    pivot = prices_df['cleaned'].median()\n",
    "    scale = prices_df['cleaned'].std()\n",
    "    prices_df['scaled'] = (prices_df['cleaned'] - pivot)/scale\n",
    "\n",
    "    prices_df['smoothed'] = prices_df['log_P'].rolling(mov_avg_period).min()\n",
    "    prices_df['roll_max'] = (prices_df['smoothed']\n",
    "                             .rolling(investment_horizon)\n",
    "                             .max()\n",
    "                             .shift(1-investment_horizon))\n",
    "    prices_df['max_return'] = prices_df['roll_max'].subtract(prices_df['log_P'])\n",
    "\n",
    "    threshold = prices_df['max_return'].mean()\n",
    "#     threshold = np.log(1+target_return)\n",
    "    \n",
    "    print(threshold)\n",
    "\n",
    "    def map_outcome(x):\n",
    "        if x == False:\n",
    "            return 0\n",
    "        elif x == True:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    prices_df['outcome'] = prices_df['max_return'].dropna().gt(threshold).map(map_outcome)\n",
    "    \n",
    "    prices_df.to_csv(processed_data_file)\n",
    "    print(prices_df)\n",
    "    print(prices_df.describe())\n",
    "\n",
    "    return prices_df[['scaled', 'outcome']].dropna(), pivot, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_samples(data_df, window, stride):\n",
    "    time_series = data_df.iloc[:, 0].values\n",
    "    outcomes = data_df.iloc[:, 1].values\n",
    "    x, y = zip(*[(time_series[i-window:i], outcomes[i]) for i in range(window, len(time_series), stride)])\n",
    "    \n",
    "    return np.array(x).reshape(-1, window, 1), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_processed_data(df, filename):\n",
    "    df.to_csv(filename, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(window):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    input_shape=(window,1)\n",
    "\n",
    "    # Convolutions\n",
    "    neurons = init_neurons\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding, input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    neurons = round(neurons * neuron_multiplier + 0.5)\n",
    "    model.add(Conv1D(neurons, conv_span, kernel_regularizer=kernel_regularizer, padding=padding))\n",
    "    model.add(LeakyReLU(alpha=leakyalpha))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pooling_span))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "#     model.add(Flatten())\n",
    "    \n",
    "#     # Recurrents\n",
    "#     model.add(GRU(128, return_sequences=True, go_backwards=True))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(GRU(128, return_sequences=False, go_backwards=False))\n",
    "#     model.add(Dropout(dropout))\n",
    "    \n",
    "    # Dense for final prediction\n",
    "    model.add(Dense(neurons, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "    neurons = round((neurons + 1) / 2 + 0.5)\n",
    "    model.add(Dense(neurons, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GaussianNoise(noise_std))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=kernel_regularizer))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.138578064736\n",
      "            P_close     log_P      diff   cleaned    scaled  smoothed  \\\n",
      "date                                                                    \n",
      "1927-12-30    17.66  2.871302       NaN       NaN       NaN       NaN   \n",
      "1928-01-03    17.76  2.876949  0.005647  0.005647  0.645242       NaN   \n",
      "1928-01-04    17.72  2.874694 -0.002255 -0.002255 -0.345389       NaN   \n",
      "1928-01-05    17.55  2.865054 -0.009640 -0.009640 -1.271310       NaN   \n",
      "1928-01-06    17.66  2.871302  0.006248  0.006248  0.720681  2.865054   \n",
      "1928-01-09    17.50  2.862201 -0.009101 -0.009101 -1.203773  2.862201   \n",
      "1928-01-10    17.37  2.854745 -0.007456 -0.007456 -0.997529  2.854745   \n",
      "1928-01-11    17.35  2.853593 -0.001152 -0.001152 -0.207135  2.853593   \n",
      "1928-01-12    17.47  2.860485  0.006893  0.006893  0.801469  2.853593   \n",
      "1928-01-13    17.58  2.866762  0.006277  0.006277  0.724257  2.853593   \n",
      "1928-01-16    17.29  2.850128 -0.016634 -0.016634 -2.148133  2.850128   \n",
      "1928-01-17    17.30  2.850707  0.000578  0.000578  0.009798  2.850128   \n",
      "1928-01-18    17.26  2.848392 -0.002315 -0.002315 -0.352914  2.848392   \n",
      "1928-01-19    17.38  2.855320  0.006928  0.006928  0.805960  2.848392   \n",
      "1928-01-20    17.48  2.861057  0.005737  0.005737  0.656615  2.848392   \n",
      "1928-01-23    17.64  2.870169  0.009112  0.009112  1.079685  2.848392   \n",
      "1928-01-24    17.71  2.874129  0.003960  0.003960  0.433842  2.848392   \n",
      "1928-01-25    17.52  2.863343 -0.010786 -0.010786 -1.415036  2.855320   \n",
      "1928-01-26    17.63  2.869602  0.006259  0.006259  0.722018  2.861057   \n",
      "1928-01-27    17.69  2.873000  0.003398  0.003398  0.363270  2.863343   \n",
      "1928-01-30    17.49  2.861629 -0.011370 -0.011370 -1.488237  2.861629   \n",
      "1928-01-31    17.57  2.866193  0.004564  0.004564  0.509470  2.861629   \n",
      "1928-02-01    17.53  2.863914 -0.002279 -0.002279 -0.348449  2.861629   \n",
      "1928-02-02    17.63  2.869602  0.005688  0.005688  0.650477  2.861629   \n",
      "1928-02-03    17.40  2.856470 -0.013132 -0.013132 -1.709094  2.856470   \n",
      "1928-02-06    17.45  2.859340  0.002869  0.002869  0.297063  2.856470   \n",
      "1928-02-07    17.44  2.858766 -0.000573 -0.000573 -0.134563  2.856470   \n",
      "1928-02-08    17.49  2.861629  0.002863  0.002863  0.296239  2.856470   \n",
      "1928-02-09    17.55  2.865054  0.003425  0.003425  0.366674  2.856470   \n",
      "1928-02-10    17.54  2.864484 -0.000570 -0.000570 -0.134153  2.858766   \n",
      "...             ...       ...       ...       ...       ...       ...   \n",
      "2017-06-22  2434.50  7.797497 -0.000456 -0.000456 -0.119845  7.796942   \n",
      "2017-06-23  2438.30  7.799056  0.001560  0.001560  0.132851  7.797497   \n",
      "2017-06-26  2439.07  7.799372  0.000316  0.000316 -0.023107  7.797497   \n",
      "2017-06-27  2419.38  7.791267 -0.008106 -0.008106 -1.078924  7.791267   \n",
      "2017-06-28  2440.69  7.800036  0.008769  0.008769  1.036781  7.791267   \n",
      "2017-06-29  2419.70  7.791399 -0.008637 -0.008637 -1.145587  7.791267   \n",
      "2017-06-30  2423.41  7.792931  0.001532  0.001532  0.129390  7.791267   \n",
      "2017-07-03  2429.01  7.795239  0.002308  0.002308  0.226688  7.791267   \n",
      "2017-07-05  2432.54  7.796691  0.001452  0.001452  0.119377  7.791399   \n",
      "2017-07-06  2409.75  7.787278 -0.009413 -0.009413 -1.242847  7.787278   \n",
      "2017-07-07  2425.18  7.793661  0.006383  0.006383  0.737543  7.787278   \n",
      "2017-07-10  2427.43  7.794588  0.000927  0.000927  0.053571  7.787278   \n",
      "2017-07-11  2425.53  7.793805 -0.000783 -0.000783 -0.160866  7.787278   \n",
      "2017-07-12  2443.25  7.801084  0.007279  0.007279  0.849920  7.787278   \n",
      "2017-07-13  2447.83  7.802957  0.001873  0.001873  0.172108  7.793661   \n",
      "2017-07-14  2459.27  7.807620  0.004663  0.004663  0.521885  7.793805   \n",
      "2017-07-17  2459.14  7.807567 -0.000053 -0.000053 -0.069322  7.793805   \n",
      "2017-07-18  2460.61  7.808165  0.000598  0.000598  0.012229  7.801084   \n",
      "2017-07-19  2473.83  7.813523  0.005358  0.005358  0.609100  7.802957   \n",
      "2017-07-20  2473.45  7.813369 -0.000154 -0.000154 -0.081954  7.807567   \n",
      "2017-07-21  2472.54  7.813001 -0.000368 -0.000368 -0.108829  7.807567   \n",
      "2017-07-24  2469.91  7.811937 -0.001064 -0.001064 -0.196124  7.808165   \n",
      "2017-07-25  2477.13  7.814856  0.002919  0.002919  0.303266  7.811937   \n",
      "2017-07-26  2477.83  7.815138  0.000283  0.000283 -0.027270  7.811937   \n",
      "2017-07-27  2475.42  7.814165 -0.000973 -0.000973 -0.184696  7.811937   \n",
      "2017-07-28  2472.10  7.812823 -0.001342 -0.001342 -0.230958  7.811937   \n",
      "2017-07-31  2470.30  7.812095 -0.000728 -0.000728 -0.154016  7.812095   \n",
      "2017-08-01  2476.35  7.814541  0.002446  0.002446  0.243986  7.812095   \n",
      "2017-08-02  2477.57  7.815034  0.000493  0.000493 -0.000942  7.812095   \n",
      "2017-08-03  2472.16  7.812848 -0.002186 -0.002186 -0.336761  7.812095   \n",
      "\n",
      "            roll_max  max_return  outcome  \n",
      "date                                       \n",
      "1927-12-30       NaN         NaN      NaN  \n",
      "1928-01-03       NaN         NaN      NaN  \n",
      "1928-01-04       NaN         NaN      NaN  \n",
      "1928-01-05       NaN         NaN      NaN  \n",
      "1928-01-06  3.177220    0.305918      1.0  \n",
      "1928-01-09  3.188417    0.326216      1.0  \n",
      "1928-01-10  3.188417    0.333672      1.0  \n",
      "1928-01-11  3.188417    0.334824      1.0  \n",
      "1928-01-12  3.188417    0.327931      1.0  \n",
      "1928-01-13  3.188417    0.321655      1.0  \n",
      "1928-01-16  3.188417    0.338288      1.0  \n",
      "1928-01-17  3.188417    0.337710      1.0  \n",
      "1928-01-18  3.188417    0.340025      1.0  \n",
      "1928-01-19  3.188417    0.333096      1.0  \n",
      "1928-01-20  3.188417    0.327359      1.0  \n",
      "1928-01-23  3.188417    0.318248      1.0  \n",
      "1928-01-24  3.201119    0.326990      1.0  \n",
      "1928-01-25  3.206398    0.343055      1.0  \n",
      "1928-01-26  3.209633    0.340031      1.0  \n",
      "1928-01-27  3.213260    0.340261      1.0  \n",
      "1928-01-30  3.221273    0.359644      1.0  \n",
      "1928-01-31  3.221273    0.355080      1.0  \n",
      "1928-02-01  3.221273    0.357359      1.0  \n",
      "1928-02-02  3.232779    0.363177      1.0  \n",
      "1928-02-03  3.232779    0.376309      1.0  \n",
      "1928-02-06  3.233173    0.373833      1.0  \n",
      "1928-02-07  3.236716    0.377949      1.0  \n",
      "1928-02-08  3.238286    0.376657      1.0  \n",
      "1928-02-09  3.238286    0.373232      1.0  \n",
      "1928-02-10  3.238286    0.373802      1.0  \n",
      "...              ...         ...      ...  \n",
      "2017-06-22       NaN         NaN      NaN  \n",
      "2017-06-23       NaN         NaN      NaN  \n",
      "2017-06-26       NaN         NaN      NaN  \n",
      "2017-06-27       NaN         NaN      NaN  \n",
      "2017-06-28       NaN         NaN      NaN  \n",
      "2017-06-29       NaN         NaN      NaN  \n",
      "2017-06-30       NaN         NaN      NaN  \n",
      "2017-07-03       NaN         NaN      NaN  \n",
      "2017-07-05       NaN         NaN      NaN  \n",
      "2017-07-06       NaN         NaN      NaN  \n",
      "2017-07-07       NaN         NaN      NaN  \n",
      "2017-07-10       NaN         NaN      NaN  \n",
      "2017-07-11       NaN         NaN      NaN  \n",
      "2017-07-12       NaN         NaN      NaN  \n",
      "2017-07-13       NaN         NaN      NaN  \n",
      "2017-07-14       NaN         NaN      NaN  \n",
      "2017-07-17       NaN         NaN      NaN  \n",
      "2017-07-18       NaN         NaN      NaN  \n",
      "2017-07-19       NaN         NaN      NaN  \n",
      "2017-07-20       NaN         NaN      NaN  \n",
      "2017-07-21       NaN         NaN      NaN  \n",
      "2017-07-24       NaN         NaN      NaN  \n",
      "2017-07-25       NaN         NaN      NaN  \n",
      "2017-07-26       NaN         NaN      NaN  \n",
      "2017-07-27       NaN         NaN      NaN  \n",
      "2017-07-28       NaN         NaN      NaN  \n",
      "2017-07-31       NaN         NaN      NaN  \n",
      "2017-08-01       NaN         NaN      NaN  \n",
      "2017-08-02       NaN         NaN      NaN  \n",
      "2017-08-03       NaN         NaN      NaN  \n",
      "\n",
      "[22504 rows x 9 columns]\n",
      "            P_close         log_P          diff       cleaned        scaled  \\\n",
      "count  22504.000000  22504.000000  22503.000000  21448.000000  21448.000000   \n",
      "mean     399.850251      4.734416      0.000220      0.000408     -0.011522   \n",
      "std      573.245719      1.723863      0.011763      0.007976      1.000000   \n",
      "min        4.400000      1.481605     -0.228997     -0.023244     -2.976913   \n",
      "25%       22.987500      3.134951     -0.004572     -0.004131     -0.580631   \n",
      "50%       96.700000      4.571613      0.000460      0.000500      0.000000   \n",
      "75%      503.140000      6.220868      0.005369      0.005104      0.577248   \n",
      "max     2477.830000      7.815138      0.153661      0.023738      2.913459   \n",
      "\n",
      "           smoothed      roll_max    max_return       outcome  \n",
      "count  22500.000000  22251.000000  22251.000000  22251.000000  \n",
      "mean       4.723103      4.839718      0.138578      0.427531  \n",
      "std        1.725022      1.689655      0.111641      0.494732  \n",
      "min        1.481605      2.169054     -0.056723      0.000000  \n",
      "25%        3.124125      3.247658      0.055959      0.000000  \n",
      "50%        4.563671      4.653770      0.116761      0.000000  \n",
      "75%        6.216027      6.298912      0.202656      1.000000  \n",
      "max        7.812095      7.812095      0.988453      1.000000  \n"
     ]
    }
   ],
   "source": [
    "full_filename = os.path.join(data_file_path, data_filename)\n",
    "data = get_price_data(full_filename, dt_format)\n",
    "processed_data, midpoint, scale = get_processed_data(data, investment_horizon)\n",
    "X, y = get_samples(processed_data, window, stride)\n",
    "save_processed_data(processed_data, model_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (18697, 2500, 1)\n",
      "y.shape:  (18697, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X.shape: ', X.shape)\n",
    "print('y.shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_data = pd.DataFrame(X.reshape(-1, window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6     \\\n",
      "0      0.720681 -1.203773 -0.997529 -0.207135  0.801469  0.724257 -2.148133   \n",
      "1     -1.203773 -0.997529 -0.207135  0.801469  0.724257 -2.148133  0.009798   \n",
      "2     -0.997529 -0.207135  0.801469  0.724257 -2.148133  0.009798 -0.352914   \n",
      "3     -0.207135  0.801469  0.724257 -2.148133  0.009798 -0.352914  0.805960   \n",
      "4      0.801469  0.724257 -2.148133  0.009798 -0.352914  0.805960  0.656615   \n",
      "5      0.724257 -2.148133  0.009798 -0.352914  0.805960  0.656615  1.079685   \n",
      "6     -2.148133  0.009798 -0.352914  0.805960  0.656615  1.079685  0.433842   \n",
      "7      0.009798 -0.352914  0.805960  0.656615  1.079685  0.433842 -1.415036   \n",
      "8     -0.352914  0.805960  0.656615  1.079685  0.433842 -1.415036  0.722018   \n",
      "9      0.805960  0.656615  1.079685  0.433842 -1.415036  0.722018  0.363270   \n",
      "10     0.656615  1.079685  0.433842 -1.415036  0.722018  0.363270 -1.488237   \n",
      "11     1.079685  0.433842 -1.415036  0.722018  0.363270 -1.488237  0.509470   \n",
      "12     0.433842 -1.415036  0.722018  0.363270 -1.488237  0.509470 -0.348449   \n",
      "13    -1.415036  0.722018  0.363270 -1.488237  0.509470 -0.348449  0.650477   \n",
      "14     0.722018  0.363270 -1.488237  0.509470 -0.348449  0.650477 -1.709094   \n",
      "15     0.363270 -1.488237  0.509470 -0.348449  0.650477 -1.709094  0.297063   \n",
      "16    -1.488237  0.509470 -0.348449  0.650477 -1.709094  0.297063 -0.134563   \n",
      "17     0.509470 -0.348449  0.650477 -1.709094  0.297063 -0.134563  0.296239   \n",
      "18    -0.348449  0.650477 -1.709094  0.297063 -0.134563  0.296239  0.366674   \n",
      "19     0.650477 -1.709094  0.297063 -0.134563  0.296239  0.366674 -0.134153   \n",
      "20    -1.709094  0.297063 -0.134563  0.296239  0.366674 -0.134153 -0.779535   \n",
      "21     0.297063 -0.134563  0.296239  0.366674 -0.134153 -0.779535 -0.350582   \n",
      "22    -0.134563  0.296239  0.366674 -0.134153 -0.779535 -0.350582 -0.423486   \n",
      "23     0.296239  0.366674 -0.134153 -0.779535 -0.350582 -0.423486 -2.323081   \n",
      "24     0.366674 -0.134153 -0.779535 -0.350582 -0.423486 -2.323081 -0.726642   \n",
      "25    -0.134153 -0.779535 -0.350582 -0.423486 -2.323081 -0.726642  1.115238   \n",
      "26    -0.779535 -0.350582 -0.423486 -2.323081 -0.726642  1.115238  0.083772   \n",
      "27    -0.350582 -0.423486 -2.323081 -0.726642  1.115238  0.083772  0.302725   \n",
      "28    -0.423486 -2.323081 -0.726642  1.115238  0.083772  0.302725 -0.574579   \n",
      "29    -2.323081 -0.726642  1.115238  0.083772  0.302725 -0.574579  0.303152   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "18667  1.112005  0.487916  0.596460  0.573133  0.371955  0.198560 -1.133922   \n",
      "18668  0.487916  0.596460  0.573133  0.371955  0.198560 -1.133922 -0.060700   \n",
      "18669  0.596460  0.573133  0.371955  0.198560 -1.133922 -0.060700 -0.862870   \n",
      "18670  0.573133  0.371955  0.198560 -1.133922 -0.060700 -0.862870  1.452309   \n",
      "18671  0.371955  0.198560 -1.133922 -0.060700 -0.862870  1.452309 -0.022054   \n",
      "18672  0.198560 -1.133922 -0.060700 -0.862870  1.452309 -0.022054 -0.359367   \n",
      "18673 -1.133922 -0.060700 -0.862870  1.452309 -0.022054 -0.359367  0.097140   \n",
      "18674 -0.060700 -0.862870  1.452309 -0.022054 -0.359367  0.097140 -0.692289   \n",
      "18675 -0.862870  1.452309 -0.022054 -0.359367  0.097140 -0.692289 -0.215347   \n",
      "18676  1.452309 -0.022054 -0.359367  0.097140 -0.692289 -0.215347  0.289225   \n",
      "18677 -0.022054 -0.359367  0.097140 -0.692289 -0.215347  0.289225  0.042789   \n",
      "18678 -0.359367  0.097140 -0.692289 -0.215347  0.289225  0.042789  0.631671   \n",
      "18679  0.097140 -0.692289 -0.215347  0.289225  0.042789  0.631671  0.461478   \n",
      "18680 -0.692289 -0.215347  0.289225  0.042789  0.631671  0.461478 -0.240134   \n",
      "18681 -0.215347  0.289225  0.042789  0.631671  0.461478 -0.240134 -0.420308   \n",
      "18682  0.289225  0.042789  0.631671  0.461478 -0.240134 -0.420308 -0.796917   \n",
      "18683  0.042789  0.631671  0.461478 -0.240134 -0.420308 -0.796917 -0.092551   \n",
      "18684  0.631671  0.461478 -0.240134 -0.420308 -0.796917 -0.092551  0.252433   \n",
      "18685  0.461478 -0.240134 -0.420308 -0.796917 -0.092551  0.252433  0.465378   \n",
      "18686 -0.240134 -0.420308 -0.796917 -0.092551  0.252433  0.465378 -0.009317   \n",
      "18687 -0.420308 -0.796917 -0.092551  0.252433  0.465378 -0.009317 -1.266209   \n",
      "18688 -0.796917 -0.092551  0.252433  0.465378 -0.009317 -1.266209  0.099839   \n",
      "18689 -0.092551  0.252433  0.465378 -0.009317 -1.266209  0.099839 -0.436935   \n",
      "18690  0.252433  0.465378 -0.009317 -1.266209  0.099839 -0.436935 -0.676869   \n",
      "18691  0.465378 -0.009317 -1.266209  0.099839 -0.436935 -0.676869  1.980540   \n",
      "18692 -0.009317 -1.266209  0.099839 -0.436935 -0.676869  1.980540  0.396936   \n",
      "18693 -1.266209  0.099839 -0.436935 -0.676869  1.980540  0.396936 -0.060725   \n",
      "18694  0.099839 -0.436935 -0.676869  1.980540  0.396936 -0.060725  1.110259   \n",
      "18695 -0.436935 -0.676869  1.980540  0.396936 -0.060725  1.110259  0.394880   \n",
      "18696 -0.676869  1.980540  0.396936 -0.060725  1.110259  0.394880 -0.107404   \n",
      "\n",
      "           7         8         9       ...         2490      2491      2492  \\\n",
      "0      0.009798 -0.352914  0.805960    ...    -0.269243 -0.788307  0.248797   \n",
      "1     -0.352914  0.805960  0.656615    ...    -0.788307  0.248797  0.557977   \n",
      "2      0.805960  0.656615  1.079685    ...     0.248797  0.557977  0.862588   \n",
      "3      0.656615  1.079685  0.433842    ...     0.557977  0.862588  0.039695   \n",
      "4      1.079685  0.433842 -1.415036    ...     0.862588  0.039695 -0.575477   \n",
      "5      0.433842 -1.415036  0.722018    ...     0.039695 -0.575477  0.347700   \n",
      "6     -1.415036  0.722018  0.363270    ...    -0.575477  0.347700  0.244222   \n",
      "7      0.722018  0.363270 -1.488237    ...     0.347700  0.244222 -0.267221   \n",
      "8      0.363270 -1.488237  0.509470    ...     0.244222 -0.267221 -0.370112   \n",
      "9     -1.488237  0.509470 -0.348449    ...    -0.267221 -0.370112 -1.611192   \n",
      "10     0.509470 -0.348449  0.650477    ...    -0.370112 -1.611192 -0.791924   \n",
      "11    -0.348449  0.650477 -1.709094    ...    -1.611192 -0.791924  0.874111   \n",
      "12     0.650477 -1.709094  0.297063    ...    -0.791924  0.874111  0.454745   \n",
      "13    -1.709094  0.297063 -0.134563    ...     0.874111  0.454745 -0.062694   \n",
      "14     0.297063 -0.134563  0.296239    ...     0.454745 -0.062694 -0.787707   \n",
      "15    -0.134563  0.296239  0.366674    ...    -0.062694 -0.787707 -0.687488   \n",
      "16     0.296239  0.366674 -0.134153    ...    -0.787707 -0.687488  1.596555   \n",
      "17     0.366674 -0.134153 -0.779535    ...    -0.687488  1.596555 -0.372135   \n",
      "18    -0.134153 -0.779535 -0.350582    ...     1.596555 -0.372135  0.349725   \n",
      "19    -0.779535 -0.350582 -0.423486    ...    -0.372135  0.349725  0.245732   \n",
      "20    -0.350582 -0.423486 -2.323081    ...     0.349725  0.245732  0.347364   \n",
      "21    -0.423486 -2.323081 -0.726642    ...     0.245732  0.347364  1.866974   \n",
      "22    -2.323081 -0.726642  1.115238    ...     0.347364  1.866974  0.740996   \n",
      "23    -0.726642  1.115238  0.083772    ...     1.866974  0.740996 -0.463895   \n",
      "24     1.115238  0.083772  0.302725    ...     0.740996 -0.463895  1.037534   \n",
      "25     0.083772  0.302725 -0.574579    ...    -0.463895  1.037534 -1.364005   \n",
      "26     0.302725 -0.574579  0.303152    ...     1.037534 -1.364005 -1.174452   \n",
      "27    -0.574579  0.303152  0.665810    ...    -1.364005 -1.174452  0.342723   \n",
      "28     0.303152  0.665810  0.227527    ...    -1.174452  0.342723 -0.773037   \n",
      "29     0.665810  0.227527 -0.062694    ...     0.342723 -0.773037 -0.062694   \n",
      "...         ...       ...       ...    ...          ...       ...       ...   \n",
      "18667 -0.060700 -0.862870  1.452309    ...    -1.218349 -1.084288 -0.288433   \n",
      "18668 -0.862870  1.452309 -0.022054    ...    -1.084288 -0.288433 -0.293682   \n",
      "18669  1.452309 -0.022054 -0.359367    ...    -0.288433 -0.293682  0.329492   \n",
      "18670 -0.022054 -0.359367  0.097140    ...    -0.293682  0.329492 -0.471828   \n",
      "18671 -0.359367  0.097140 -0.692289    ...     0.329492 -0.471828  0.663400   \n",
      "18672  0.097140 -0.692289 -0.215347    ...    -0.471828  0.663400  0.276877   \n",
      "18673 -0.692289 -0.215347  0.289225    ...     0.663400  0.276877 -0.269933   \n",
      "18674 -0.215347  0.289225  0.042789    ...     0.276877 -0.269933  1.601726   \n",
      "18675  0.289225  0.042789  0.631671    ...    -0.269933  1.601726 -2.352327   \n",
      "18676  0.042789  0.631671  0.461478    ...     1.601726 -2.352327  2.145684   \n",
      "18677  0.631671  0.461478 -0.240134    ...    -2.352327  2.145684  2.054795   \n",
      "18678  0.461478 -0.240134 -0.420308    ...     2.145684  2.054795  1.626589   \n",
      "18679 -0.240134 -0.420308 -0.796917    ...     2.054795  1.626589  0.181384   \n",
      "18680 -0.420308 -0.796917 -0.092551    ...     1.626589  0.181384 -0.924156   \n",
      "18681 -0.796917 -0.092551  0.252433    ...     0.181384 -0.924156  0.606649   \n",
      "18682 -0.092551  0.252433  0.465378    ...    -0.924156  0.606649 -0.172011   \n",
      "18683  0.252433  0.465378 -0.009317    ...     0.606649 -0.172011  1.835259   \n",
      "18684  0.465378 -0.009317 -1.266209    ...    -0.172011  1.835259  0.363935   \n",
      "18685 -0.009317 -1.266209  0.099839    ...     1.835259  0.363935  0.813033   \n",
      "18686 -1.266209  0.099839 -0.436935    ...     0.363935  0.813033 -0.045801   \n",
      "18687  0.099839 -0.436935 -0.676869    ...     0.813033 -0.045801  0.594948   \n",
      "18688 -0.436935 -0.676869  1.980540    ...    -0.045801  0.594948 -0.179214   \n",
      "18689 -0.676869  1.980540  0.396936    ...     0.594948 -0.179214  0.235637   \n",
      "18690  1.980540  0.396936 -0.060725    ...    -0.179214  0.235637 -0.242766   \n",
      "18691  0.396936 -0.060725  1.110259    ...     0.235637 -0.242766  0.471556   \n",
      "18692 -0.060725  1.110259  0.394880    ...    -0.242766  0.471556 -0.516430   \n",
      "18693  1.110259  0.394880 -0.107404    ...     0.471556 -0.516430  0.506958   \n",
      "18694  0.394880 -0.107404  0.373036    ...    -0.516430  0.506958 -0.440825   \n",
      "18695 -0.107404  0.373036 -0.851806    ...     0.506958 -0.440825 -0.022228   \n",
      "18696  0.373036 -0.851806  0.088321    ...    -0.440825 -0.022228 -0.213060   \n",
      "\n",
      "           2493      2494      2495      2496      2497      2498      2499  \n",
      "0      0.557977  0.862588  0.039695 -0.575477  0.347700  0.244222 -0.267221  \n",
      "1      0.862588  0.039695 -0.575477  0.347700  0.244222 -0.267221 -0.370112  \n",
      "2      0.039695 -0.575477  0.347700  0.244222 -0.267221 -0.370112 -1.611192  \n",
      "3     -0.575477  0.347700  0.244222 -0.267221 -0.370112 -1.611192 -0.791924  \n",
      "4      0.347700  0.244222 -0.267221 -0.370112 -1.611192 -0.791924  0.874111  \n",
      "5      0.244222 -0.267221 -0.370112 -1.611192 -0.791924  0.874111  0.454745  \n",
      "6     -0.267221 -0.370112 -1.611192 -0.791924  0.874111  0.454745 -0.062694  \n",
      "7     -0.370112 -1.611192 -0.791924  0.874111  0.454745 -0.062694 -0.787707  \n",
      "8     -1.611192 -0.791924  0.874111  0.454745 -0.062694 -0.787707 -0.687488  \n",
      "9     -0.791924  0.874111  0.454745 -0.062694 -0.787707 -0.687488  1.596555  \n",
      "10     0.874111  0.454745 -0.062694 -0.787707 -0.687488  1.596555 -0.372135  \n",
      "11     0.454745 -0.062694 -0.787707 -0.687488  1.596555 -0.372135  0.349725  \n",
      "12    -0.062694 -0.787707 -0.687488  1.596555 -0.372135  0.349725  0.245732  \n",
      "13    -0.787707 -0.687488  1.596555 -0.372135  0.349725  0.245732  0.347364  \n",
      "14    -0.687488  1.596555 -0.372135  0.349725  0.245732  0.347364  1.866974  \n",
      "15     1.596555 -0.372135  0.349725  0.245732  0.347364  1.866974  0.740996  \n",
      "16    -0.372135  0.349725  0.245732  0.347364  1.866974  0.740996 -0.463895  \n",
      "17     0.349725  0.245732  0.347364  1.866974  0.740996 -0.463895  1.037534  \n",
      "18     0.245732  0.347364  1.866974  0.740996 -0.463895  1.037534 -1.364005  \n",
      "19     0.347364  1.866974  0.740996 -0.463895  1.037534 -1.364005 -1.174452  \n",
      "20     1.866974  0.740996 -0.463895  1.037534 -1.364005 -1.174452  0.342723  \n",
      "21     0.740996 -0.463895  1.037534 -1.364005 -1.174452  0.342723 -0.773037  \n",
      "22    -0.463895  1.037534 -1.364005 -1.174452  0.342723 -0.773037 -0.062694  \n",
      "23     1.037534 -1.364005 -1.174452  0.342723 -0.773037 -0.062694 -1.804755  \n",
      "24    -1.364005 -1.174452  0.342723 -0.773037 -0.062694 -1.804755  0.040453  \n",
      "25    -1.174452  0.342723 -0.773037 -0.062694 -1.804755  0.040453 -1.306094  \n",
      "26     0.342723 -0.773037 -0.062694 -1.804755  0.040453 -1.306094 -0.062694  \n",
      "27    -0.773037 -0.062694 -1.804755  0.040453 -1.306094 -0.062694  0.974328  \n",
      "28    -0.062694 -1.804755  0.040453 -1.306094 -0.062694  0.974328  0.863348  \n",
      "29    -1.804755  0.040453 -1.306094 -0.062694  0.974328  0.863348 -0.576317  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "18667 -0.293682  0.329492 -0.471828  0.663400  0.276877 -0.269933  1.601726  \n",
      "18668  0.329492 -0.471828  0.663400  0.276877 -0.269933  1.601726 -2.352327  \n",
      "18669 -0.471828  0.663400  0.276877 -0.269933  1.601726 -2.352327  2.145684  \n",
      "18670  0.663400  0.276877 -0.269933  1.601726 -2.352327  2.145684  2.054795  \n",
      "18671  0.276877 -0.269933  1.601726 -2.352327  2.145684  2.054795  1.626589  \n",
      "18672 -0.269933  1.601726 -2.352327  2.145684  2.054795  1.626589  0.181384  \n",
      "18673  1.601726 -2.352327  2.145684  2.054795  1.626589  0.181384 -0.924156  \n",
      "18674 -2.352327  2.145684  2.054795  1.626589  0.181384 -0.924156  0.606649  \n",
      "18675  2.145684  2.054795  1.626589  0.181384 -0.924156  0.606649 -0.172011  \n",
      "18676  2.054795  1.626589  0.181384 -0.924156  0.606649 -0.172011  1.835259  \n",
      "18677  1.626589  0.181384 -0.924156  0.606649 -0.172011  1.835259  0.363935  \n",
      "18678  0.181384 -0.924156  0.606649 -0.172011  1.835259  0.363935  0.813033  \n",
      "18679 -0.924156  0.606649 -0.172011  1.835259  0.363935  0.813033 -0.045801  \n",
      "18680  0.606649 -0.172011  1.835259  0.363935  0.813033 -0.045801  0.594948  \n",
      "18681 -0.172011  1.835259  0.363935  0.813033 -0.045801  0.594948 -0.179214  \n",
      "18682  1.835259  0.363935  0.813033 -0.045801  0.594948 -0.179214  0.235637  \n",
      "18683  0.363935  0.813033 -0.045801  0.594948 -0.179214  0.235637 -0.242766  \n",
      "18684  0.813033 -0.045801  0.594948 -0.179214  0.235637 -0.242766  0.471556  \n",
      "18685 -0.045801  0.594948 -0.179214  0.235637 -0.242766  0.471556 -0.516430  \n",
      "18686  0.594948 -0.179214  0.235637 -0.242766  0.471556 -0.516430  0.506958  \n",
      "18687 -0.179214  0.235637 -0.242766  0.471556 -0.516430  0.506958 -0.440825  \n",
      "18688  0.235637 -0.242766  0.471556 -0.516430  0.506958 -0.440825 -0.022228  \n",
      "18689 -0.242766  0.471556 -0.516430  0.506958 -0.440825 -0.022228 -0.213060  \n",
      "18690  0.471556 -0.516430  0.506958 -0.440825 -0.022228 -0.213060  0.138524  \n",
      "18691 -0.516430  0.506958 -0.440825 -0.022228 -0.213060  0.138524  0.141663  \n",
      "18692  0.506958 -0.440825 -0.022228 -0.213060  0.138524  0.141663 -0.221994  \n",
      "18693 -0.440825 -0.022228 -0.213060  0.138524  0.141663 -0.221994 -0.862827  \n",
      "18694 -0.022228 -0.213060  0.138524  0.141663 -0.221994 -0.862827  0.329610  \n",
      "18695 -0.213060  0.138524  0.141663 -0.221994 -0.862827  0.329610 -0.036043  \n",
      "18696  0.138524  0.141663 -0.221994 -0.862827  0.329610 -0.036043  1.011351  \n",
      "\n",
      "[18697 rows x 2500 columns]\n"
     ]
    }
   ],
   "source": [
    "print(check_data)\n",
    "(check_data[-500:] * scale + midpoint).to_csv('samples.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_close</th>\n",
       "      <th>log_P</th>\n",
       "      <th>diff</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>scaled</th>\n",
       "      <th>smoothed</th>\n",
       "      <th>roll_max</th>\n",
       "      <th>max_return</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22504.000000</td>\n",
       "      <td>22504.000000</td>\n",
       "      <td>22503.000000</td>\n",
       "      <td>21448.000000</td>\n",
       "      <td>21448.000000</td>\n",
       "      <td>22500.000000</td>\n",
       "      <td>22251.000000</td>\n",
       "      <td>22251.000000</td>\n",
       "      <td>22251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>399.850251</td>\n",
       "      <td>4.734416</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.011522</td>\n",
       "      <td>4.723103</td>\n",
       "      <td>4.839718</td>\n",
       "      <td>0.138578</td>\n",
       "      <td>0.427531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>573.245719</td>\n",
       "      <td>1.723863</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.725022</td>\n",
       "      <td>1.689655</td>\n",
       "      <td>0.111641</td>\n",
       "      <td>0.494732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.481605</td>\n",
       "      <td>-0.228997</td>\n",
       "      <td>-0.023244</td>\n",
       "      <td>-2.976913</td>\n",
       "      <td>1.481605</td>\n",
       "      <td>2.169054</td>\n",
       "      <td>-0.056723</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.987500</td>\n",
       "      <td>3.134951</td>\n",
       "      <td>-0.004572</td>\n",
       "      <td>-0.004131</td>\n",
       "      <td>-0.580631</td>\n",
       "      <td>3.124125</td>\n",
       "      <td>3.247658</td>\n",
       "      <td>0.055959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.700000</td>\n",
       "      <td>4.571613</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.563671</td>\n",
       "      <td>4.653770</td>\n",
       "      <td>0.116761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>503.140000</td>\n",
       "      <td>6.220868</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.577248</td>\n",
       "      <td>6.216027</td>\n",
       "      <td>6.298912</td>\n",
       "      <td>0.202656</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2477.830000</td>\n",
       "      <td>7.815138</td>\n",
       "      <td>0.153661</td>\n",
       "      <td>0.023738</td>\n",
       "      <td>2.913459</td>\n",
       "      <td>7.812095</td>\n",
       "      <td>7.812095</td>\n",
       "      <td>0.988453</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            P_close         log_P          diff       cleaned        scaled  \\\n",
       "count  22504.000000  22504.000000  22503.000000  21448.000000  21448.000000   \n",
       "mean     399.850251      4.734416      0.000220      0.000408     -0.011522   \n",
       "std      573.245719      1.723863      0.011763      0.007976      1.000000   \n",
       "min        4.400000      1.481605     -0.228997     -0.023244     -2.976913   \n",
       "25%       22.987500      3.134951     -0.004572     -0.004131     -0.580631   \n",
       "50%       96.700000      4.571613      0.000460      0.000500      0.000000   \n",
       "75%      503.140000      6.220868      0.005369      0.005104      0.577248   \n",
       "max     2477.830000      7.815138      0.153661      0.023738      2.913459   \n",
       "\n",
       "           smoothed      roll_max    max_return       outcome  \n",
       "count  22500.000000  22251.000000  22251.000000  22251.000000  \n",
       "mean       4.723103      4.839718      0.138578      0.427531  \n",
       "std        1.725022      1.689655      0.111641      0.494732  \n",
       "min        1.481605      2.169054     -0.056723      0.000000  \n",
       "25%        3.124125      3.247658      0.055959      0.000000  \n",
       "50%        4.563671      4.653770      0.116761      0.000000  \n",
       "75%        6.216027      6.298912      0.202656      1.000000  \n",
       "max        7.812095      7.812095      0.988453      1.000000  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2673a6f8208>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGtBJREFUeJzt3XucXGWd5/HPt6q7k849kE7IzSRCAgaFiL1c1lEDOgqo\nMDMvXgq7I+iyMuMOuo7OzOLqOixzcUZfo64L48gIIqyC6FyMioMYYWRGiXQ0gEmMhAhJk1vnHtJJ\n+lK//eOcjtVFd7o6VKdTT77v16teOeep55zznCdV3zr9nDqnFBGYmVlaCqPdADMzqz2Hu5lZghzu\nZmYJcribmSXI4W5mliCHu5lZghzuZjUg6VlJbzrGZV8naV2t22QnN4e7HReSHpG0W9KYYSwTks4Y\nyXaNhsr9iohHI+LM0WyTpcfhbiNO0nzgdUAAV4xqY4YgqaGaMrMTncPdjodrgceAu4Dr+grzo/n/\nWjb/bkn/lk//MC9+QtILkt6Zl79X0npJuyQtkzSrbPmzJT2UP7dN0v/My8dI+qykzfnjs31/QUha\nKqld0v+QtBX40kBled23SVolaY+kH0k6Z6CdlXS+pB/n9bZIulVS02D71be9suVfkffNHkmrJV1R\n9txdkm6T9B1J+yWtkHT6sf23WMoc7nY8XAt8JX+8RdKMoRaIiNfnk+dGxISI+JqkS4BPAO8AZgLP\nAfcBSJoIfB/4F2AWcAawPF/HR4ELgSXAucD5wMfKNncacAowD7hhoDJJ5wF3Ar8HnAp8AVg2yDBT\nL/CHwDTgIuCNwH8bbL/KF5TUCHwL+B4wHXg/8BVJ5cM21wD/G5gKrAf+YsBOtJOaw91GlKTfIAvI\n+yNiJfAM8J+OcXX/GbgzIn4aEYeBjwAX5cM+bwO2RsTfRMShiNgfESvKlrslIrZHRAdZML6rbL0l\n4E8j4nBEHByk7L3AFyJiRUT0RsSXgcNkHxr9RMTKiHgsInoi4lmyD4I3VLmPFwITgL+KiK6I+AHw\nbbJA7/OPEfGTiOgh+8BcUuW67STicLeRdh3wvYjYkc9/lbKhmWGaRXa0DkBEvADsBGYDc8k+OIZc\nLp+eVTbfERGHKpapLJsHfDgfKtkjaU++zVkVyyFpkaRvS9oqaR/wl2RH8dWYBWyKiFJFe2eXzW8t\nm+4k+zAw68cnimzESGomG0Ip5mPXAGOAKZLOBQ4A48oWOW2IVW4mC9m+9Y8nGyJ5HthE/6PbgZZb\nnc+/LC/rM9CtUSvLNgF/ERHVDIF8HvgZcE1E7Jf0QeCqKpbra+tcSYWygH8Z8MsqlzcDfORuI+u3\nyMafF5MNHSwBXgE8SjYOvwr4HUnj8q8GXl+x/Dbg5WXzXwXeI2lJPtb9l8CKfOjj28Bpkj6Yn0Cd\nKOmCfLl7gY9JapE0Dfg48P+GuS9/D/y+pAuUGS/prflYf6WJwD7gBUlnAe8bYr/KrSD70PsTSY2S\nlgJvJz+3YFYth7uNpOuAL0XExojY2vcAbiUbB/8M0EUWdl8mGz8udzPw5XwY5B0RsRz4X8A/AFuA\n04GrASJiP/CbZEG4FXgauDhfz58DbcCTwFPAT/OyqkVEG9m4+63AbrITme8epPofkZ1X2E/2ofC1\niuf77VfFdrrIvi56GbAD+Fvg2oj4xXDaayb/WIeZWXp85G5mliCHu5lZghzuZmYJcribmSVo1L7n\nPm3atJg/f/5obd7MrC6tXLlyR0S0DFVv1MJ9/vz5tLW1jdbmzczqkqTnhq7lYRkzsyQ53M3MEuRw\nNzNLkMPdzCxBDnczswQNGe6S7pS0XdLPB3lekj6X//TZk/kv1oyI3lKwfO02Prf8aZav3UZvyffF\nMTMbSDVfhbyL7E54dw/y/GXAwvxxAdm9rC8YpO4x6y0F77pjBas27eFgVy/NTUWWzJ3CPddfQLGg\nWm/OzKyuDXnkHhE/BHYdpcqVwN2ReYzshxhm1qqBfR5Zt51Vm/bQ2dVLAJ1dvazatIdH1m2v9abM\nzOpeLcbcZ5P9Sk2fdvr/JNgRkm6Q1CapraOjY1gbWb15Hwe7evuVHezqZc3mfcNsrplZ+moR7gON\niQw4GB4Rt0dEa0S0trQMefVsP2fPmkRzU7FfWXNTkcWzJg1rPWZmJ4NahHs72Q8F95lD/9+nrIml\nZ05nydwpqLcLosS4fMx96ZnTa70pM7O6V4twXwZcm39r5kJgb0RsqcF6+ykWxD3XX0DL099iSvu/\n83+vebVPppqZDWLIb8tIuhdYCkyT1A78KdAIEBF/BzwAXE72m5KdwHtGqrHFghi3ZwPj9mzgja+Y\nMVKbMTOre0OGe0RcM8TzAfxBzVpkZmYvma9QNTNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxB\nDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNL\nkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3M\nEuRwNzNLkMPdzCxBVYW7pEslrZO0XtJNAzz/MkkPS/qZpCclXV77ppqZWbWGDHdJReA24DJgMXCN\npMUV1T4G3B8RrwauBv621g01M7PqVXPkfj6wPiI2REQXcB9wZUWdACbl05OBzbVropmZDVc14T4b\n2FQ2356XlbsZ+F1J7cADwPsHWpGkGyS1SWrr6Og4huaamVk1qgl3DVAWFfPXAHdFxBzgcuAeSS9a\nd0TcHhGtEdHa0tIy/NaamVlVqgn3dmBu2fwcXjzscj1wP0BE/BgYC0yrRQPNzGz4qgn3x4GFkhZI\naiI7Ybqsos5G4I0Akl5BFu4edzEzGyVDhntE9AA3Ag8Ca8m+FbNa0i2SrsirfRh4r6QngHuBd0dE\n5dCNmZkdJw3VVIqIB8hOlJaXfbxseg3w2to2zczMjpWvUDUzS5DD3cwsQQ53M7MEOdzNzBLkcDcz\nS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzN\nzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53\nM7MEOdzNzBLkcDczS5DD3cwsQVWFu6RLJa2TtF7STYPUeYekNZJWS/pqbZtpZmbD0TBUBUlF4Dbg\nN4F24HFJyyJiTVmdhcBHgNdGxG5J00eqwWZmNrRqjtzPB9ZHxIaI6ALuA66sqPNe4LaI2A0QEdtr\n20wzMxuOasJ9NrCpbL49Lyu3CFgk6d8lPSbp0oFWJOkGSW2S2jo6Oo6txWZmNqRqwl0DlEXFfAOw\nEFgKXAN8UdKUFy0UcXtEtEZEa0tLy3DbamZmVaom3NuBuWXzc4DNA9T5ZkR0R8SvgHVkYW9mZqOg\nmnB/HFgoaYGkJuBqYFlFnX8GLgaQNI1smGZDLRtqZmbVGzLcI6IHuBF4EFgL3B8RqyXdIumKvNqD\nwE5Ja4CHgT+OiJ0j1WgzMzu6Ib8KCRARDwAPVJR9vGw6gA/lDzMzG2W+QtXMLEEOdzOzBDnczcwS\n5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOz\nBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93MLEEOdzOzBDnczcwS5HA3M0uQw93M\nLEEOdzOzBDnczcwS5HA3M0uQw93MLEFVhbukSyWtk7Re0k1HqXeVpJDUWrsmmpnZcA0Z7pKKwG3A\nZcBi4BpJiweoNxH4ALCi1o00M7PhqebI/XxgfURsiIgu4D7gygHq/RnwSeBQDdtnZmbHoJpwnw1s\nKptvz8uOkPRqYG5EfPtoK5J0g6Q2SW0dHR3DbqyZmVWnmnDXAGVx5EmpAHwG+PBQK4qI2yOiNSJa\nW1paqm+lmZkNSzXh3g7MLZufA2wum58IvBJ4RNKzwIXAMp9UNTMbPdWE++PAQkkLJDUBVwPL+p6M\niL0RMS0i5kfEfOAx4IqIaBuRFpuZ2ZCGDPeI6AFuBB4E1gL3R8RqSbdIumKkG2hmZsPXUE2liHgA\neKCi7OOD1F360ptlZmYvha9QNTNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3M\nEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDncz\nswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPd\nzCxBVYW7pEslrZO0XtJNAzz/IUlrJD0pabmkebVvqpmZVWvIcJdUBG4DLgMWA9dIWlxR7WdAa0Sc\nA3wD+GStG2pmZtWr5sj9fGB9RGyIiC7gPuDK8goR8XBEdOazjwFzattMMzMbjmrCfTawqWy+PS8b\nzPXAdwd6QtINktoktXV0dFTfSjMzG5Zqwl0DlMWAFaXfBVqBTw30fETcHhGtEdHa0tJSfSvNzGxY\nGqqo0w7MLZufA2yurCTpTcBHgTdExOHaNM/MzI5FNUfujwMLJS2Q1ARcDSwrryDp1cAXgCsiYnvt\nm2lmZsMxZLhHRA9wI/AgsBa4PyJWS7pF0hV5tU8BE4CvS1oladkgqzMzs+OgmmEZIuIB4IGKso+X\nTb+pxu0yM7OXwFeompklyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5kl\nyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZmCXK4m5klyOFuZpYgh7uZWYIc7mZm\nCaqrcO8tBZ1TXs6e2RexfO02eksx2k0yMzshNYx2A6rVWwredccKOha+nSg08P57f8aSuVO45/oL\nKBY02s0zMzuh1M2R+yPrtrNq0x6i2AQq0NnVy6pNe3hk3fbRbpqZ2QmnbsJ99eZ9HOzq7Vd2sKuX\nNZv3jVKLzMxOXHUT7mfPmkRzU7FfWXNTkcWzJo1Si8zMTlx1E+5Lz5zOkrlTiK5DRKkEQER29O4T\nq2Zm/SlidIKxtbU12trahrXM3s5uzrn5u6AC0q9PojY3FvjUVedy2atm9ju52lsKHlm3ndWb9/GK\n0yaCsuGdrp5e2p7bzb6DPbxp8XQ+cMkimhoK/eqfPWsSS8+cTrGgQcsH2s7ZsybxuoUt/Ou67Xzn\nqS0AvPWcmVxy1owhT/wOtR37NfeVnawkrYyI1iHr1Uu495aC1j97kF2dPf2CvVxjUVy88FSeeH4v\n2/Z3AyBgqD0sAPNPHcuGnYf6lTc3FFh65jR+8uxudh3oPrIeAYWCmDd1LEvPmsG9P9nIwe7SkPsw\nY2ITk5obKUgsbJlAFGDn/sO0TBzLy04dxz//7Hk27zl0ZDvNDQUuOWs6Z8yYyCtnTaIUwXee3My6\nbS8giaVnTSNK8J0ntxIRLJk7hTNmTOScOZOH/ABaeuZ0unpK/PHXV/HYhl1MHdfAGTMmsftA1p6X\nt0zgnDmTed3CFh5eu407/m0Dz+zoZGyDWDJ3KvNbxrNxZyc7XjjMa+ZN5eyZk3lwzVYg+zB7w6Lp\nL/qAe8Oi6Tz6dEe/NvSWgs8t/yUPrdkGwJkzJvHWc06jUBBrt+w/Ug+yk+pPPb+Xnt4S31y1me37\nD9PVU6K5qfiib071loIfrN12ZPtvWTyDp7bsZfma7AT8oukTmdcyjoZCgbNnTjrywd/TW2Ljrk4K\n0pEP5d5ScOsPnqbtud20zpvKjZcspKnhxX/0DnQwUb4P1RwoDPZ/NdAHV+U+VnsQMZCR/LDs6ikN\nu//8gT245ML9oTVbee/dK0ewRZYC5Y+hP2prq0HQc4xvpQJZewtkbe+teL5R2YFLoVigsSCmjm9k\n/8EuOg5U1vx1/eZG0dkVhGBsY5HpExrY8UIX+7uCgqBlQhPnzpnMU+176NjfTU8VbZw0tkhPCSaM\nLTJhTAOdh3vpjV72HezlUL7zBaCxAQ4NtcKy/S0BY4qiWIDO7jhycDNxTJGli6Yzr2UcBYlSKfhV\nx35WPrebHS9kB1vNTQ2cddoELlhwKsWiIKAnSqxYv4PVm/fTGzB1XAPFYoGunhLzp41j5uRxFAVz\nT80+3F81OzuIqTzwOFE/WJIL9/d/5ad8Kz9CMTM70fR9SDc3FBg/poGWiWOY2tzA0x0H6O4NXjV7\nEh944yJOmzyWuaeMO+btVBvuVV3EJOlS4P8AReCLEfFXFc+PAe4GXgPsBN4ZEc8Ot9FH07ZxVy1X\nZ2ZWU31/LR7sKXGwp4sdB7r6Pf/o+p08uv7H/coe/tBSFkwfPyLtGfLbMpKKwG3AZcBi4BpJiyuq\nXQ/sjogzgM8Af13rhu6u6Cgzs3p38acf4VfbD4zIuqs5cj8fWB8RGwAk3QdcCawpq3MlcHM+/Q3g\nVkmKo4z5bOg4wDu/8OPBnn6Rw8c6oGlmdgK7+NOPcMGCU2q+3mq+5z4b2FQ2356XDVgnInqAvcCp\nlSuSdIOkNklt3d3dw2roiXlqw8zsxFTNkftAuVp5GF1NHSLiduB2yE6ofu33Lqpi85mLPvEQW/Z6\naMbM0lIQDCcL7//9KtdbRZ12YG7Z/Bxg82B1JDUAk4GangGdPbm5lqszMzshLP/DpSOy3mrC/XFg\noaQFkpqAq4FlFXWWAdfl01cBPzjaePuxaGgoDl3JzKyOjOS3ZYYclomIHkk3Ag+SfRXyzohYLekW\noC0ilgF3APdIWk92xH51rRt6/vxTeGyDvw5pZqNnoCveGwowubmJK5bM5MmNe1izdR9jG4uc0TKR\nmVOyq88bC0VeOfv4XhxVNxcxdfWUaP3z77Hv0MBX5Vn9KQLjm7IrKau4oLGfam4rAdnRS1OjEKK7\nu8Rwz9pUbqe5CC0Tm9i4p/o1NQoaitlVm0O1uVHZGGxXKesfBrny9fRp4zhlwhie2b6fXZ39e6+p\nKKaNb2LWlGaKBWXbjGD3wW46D/fQ3FSkWCgypbmB8+ZNZfOeQ0Aw+5RmNu3o5JfbX6BUKlEC9nb2\ncOqEJt73+tN5cO0W/vUX2zmY70cBmDBG9Ibo7ilRLEB3D/SQtV0F0TKhibedO4vt+w4DwdxTx1FU\ngX95ajPrKr4COKYIb1jUwhMbd7PjQA+lvD9C0F2CxgJcfNYMFs+cxKvKbo3xxUefYe3WF+ju7WX8\nmEamTWjijBkTaCgU+t1GoqunxJ984wmeaN/DuXOm8InfOYcVv9rJms37WHyCX5VaLrkrVCEL+M9+\nfx13/eg5Dnb19nujjClmlycXBL2RPcY2wKSxjezu7Kar4nr0xgJMHNvItAljWDRjAis37hr0hK2A\nWZPHct68KXTs72L6pDHMmjqWnz67h007D7DjQBeD3VpmanMD7/gPL+OJjbvY2dnNoe4SS+ZO4ZNX\nnXvkFsZ99wj51hPP84ut+zlwuJfxYxs467RJvP3cX98vpLcUfH/1Vr70o2fZ09nF6TMmsLBl4pEX\n+qNPdxx5ofbN//z5vfSWgmKhwCtnTxqwXt89YCJ+/ebrLZXYtOsgfW9IIZ7dcYAd+w8zbeIYFkzL\n/pSUREQM+m/fdv/j6dP4/CPrWfncbl4zxP1FhnO/lL77kVTu51Bv1L7lyt/YQFVllffs6fs/2Xuw\nmzeW3YiuGgO1Y7B7zQxUp5b3lzme6rXdJ4Ikw71PNW+I4S5T+XxlCB5tG+XLnpXfMOoXW/bX1dGA\nmdWHpMPdzOxkVW24182PdZiZWfUc7mZmCXK4m5klyOFuZpYgh7uZWYJG7dsykjqA545x8WnAjho2\nJzXun6Nz/xyd+2dwJ0LfzIuIlqEqjVq4vxSS2qr5KtDJyv1zdO6fo3P/DK6e+sbDMmZmCXK4m5kl\nqF7D/fbRbsAJzv1zdO6fo3P/DK5u+qYux9zNzOzo6vXI3czMjsLhbmaWoLoLd0mXSlonab2km0a7\nPceLpGclPSVplaS2vOwUSQ9Jejr/d2peLkmfy/voSUnnla3nurz+05KuG2x7JzpJd0raLunnZWU1\n6w9Jr8n7e32+bF3dt3mQ/rlZ0vP5a2iVpMvLnvtIvq/rJL2lrHzA91v+s5sr8n77Wv4TnHVD0lxJ\nD0taK2m1pP+el6fzGoqIunmQ/cDLM8DLgSbgCWDxaLfrOO37s8C0irJPAjfl0zcBf51PXw58l+x3\nRi4EVuTlpwAb8n+n5tNTR3vfjrE/Xg+cB/x8JPoD+AlwUb7Md4HLRnufa9A/NwN/NEDdxfl7aQyw\nIH+PFY/2fgPuB67Op/8OeN9o7/Mw+2cmcF4+PRH4Zd4PybyG6u3I/XxgfURsiIgu4D7gylFu02i6\nEvhyPv1l4LfKyu+OzGPAFEkzgbcAD0XErojYDTwEXHq8G10LEfFDst/rLVeT/sifmxQRP47sXXp3\n2brqwiD9M5grgfsi4nBE/ApYT/ZeG/D9lh+BXgJ8I1++vK/rQkRsiYif5tP7gbXAbBJ6DdVbuM8G\nNpXNt+dlJ4MAvidppaQb8rIZEbEFshcrMD0vH6yfUu+/WvXH7Hy6sjwFN+bDCnf2DTkw/P45FdgT\nET0V5XVJ0nzg1cAKEnoN1Vu4DzRmdbJ8l/O1EXEecBnwB5Jef5S6g/XTydp/w+2PVPvp88DpwBJg\nC/A3eflJ2z+SJgD/AHwwIvYdreoAZSd0H9VbuLcDc8vm5wCbR6ktx1VEbM7/3Q78E9mfzNvyP//I\n/92eVx+sn1Lvv1r1R3s+XVle1yJiW0T0RkQJ+Huy1xAMv392kA1LNFSU1xVJjWTB/pWI+Me8OJnX\nUL2F++PAwvxMfRNwNbBslNs04iSNlzSxbxp4M/Bzsn3vOzt/HfDNfHoZcG1+hv9CYG/+J+aDwJsl\nTc3/JH9zXpaKmvRH/tx+SRfm48vXlq2rbvWFVu63yV5DkPXP1ZLGSFoALCQ7GTjg+y0fQ34YuCpf\nvryv60L+/3oHsDYiPl32VDqvodE+az3cB9lZ61+SncX/6Gi35zjt88vJvqnwBLC6b7/Jxj6XA0/n\n/56Slwu4Le+jp4DWsnX9F7ITZuuB94z2vr2EPrmXbGihm+wo6fpa9gfQShZ+zwC3kl/NXS+PQfrn\nnnz/nyQLq5ll9T+a7+s6yr7VMdj7LX9N/iTvt68DY0Z7n4fZP79BNkzyJLAqf1ye0mvItx8wM0tQ\nvQ3LmJlZFRzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXo/wPoTXUJH5OAfQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2673a6f8208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_acf(processed_data['scaled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 2499, 8)           24        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)   (None, 2499, 8)           0         \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 2499, 8)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 1249, 8)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_93 (GaussianN (None, 1249, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 1248, 16)          272       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_81 (LeakyReLU)   (None, 1248, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 1248, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 624, 16)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_94 (GaussianN (None, 624, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 623, 32)           1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_82 (LeakyReLU)   (None, 623, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 623, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 311, 32)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_95 (GaussianN (None, 311, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 310, 64)           4160      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_83 (LeakyReLU)   (None, 310, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 310, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 155, 64)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_96 (GaussianN (None, 155, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 154, 128)          16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)   (None, 154, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 154, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 77, 128)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_97 (GaussianN (None, 77, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 76, 256)           65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)   (None, 76, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 76, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 38, 256)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_98 (GaussianN (None, 38, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 37, 512)           262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)   (None, 37, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 37, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 18, 512)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_99 (GaussianN (None, 18, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 17, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)   (None, 17, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 17, 1024)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8, 1024)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_100 (Gaussian (None, 8, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 2048)           4196352   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)   (None, 7, 2048)           0         \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 7, 2048)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 3, 2048)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_101 (Gaussian (None, 3, 2048)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_90 (Conv1D)           (None, 2, 4096)           16781312  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)   (None, 2, 4096)           0         \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 2, 4096)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 1, 4096)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_102 (Gaussian (None, 1, 4096)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_103 (Gaussian (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2049)              8394753   \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 2049)              0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_104 (Gaussian (None, 2049)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 2050      \n",
      "=================================================================\n",
      "Total params: 47,555,851\n",
      "Trainable params: 47,555,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(window)\n",
    "model.summary()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14957 samples, validate on 3740 samples\n",
      "Epoch 1/10000\n",
      "  768/14957 [>.............................] - ETA: 1407s - loss: 6.5679 - acc: 0.4609"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-93a14db9aeaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m training_history = model.fit(X, y, batch_size=batch_size, epochs=num_epochs, verbose=1,\n\u001b[1;32m----> 5\u001b[1;33m           callbacks=[checkpointer, earlystopper], validation_split=validation_frac)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Miniconda3\\envs\\dlnd\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=model_file, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "training_history = model.fit(X, y, batch_size=batch_size, epochs=num_epochs, verbose=1,\n",
    "          callbacks=[checkpointer, earlystopper], validation_split=validation_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([np.max(np.abs(weights)) for weights in model.get_weights()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(predictions.squeeze()).to_csv(predictions_file, sep=',')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
